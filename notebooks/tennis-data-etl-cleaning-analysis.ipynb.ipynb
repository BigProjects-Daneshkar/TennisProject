{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Reading Section:\n",
    "##### In this section, we will use the following codes, which will be explained below, to read the data we need from the main reference file and convert it to CSV format so that we can use it in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è IMPORTANT ‚Äî READ BEFORE RUNNING\n",
    "\n",
    "This notebook expects the raw dataset to be available **before execution**.  \n",
    "If the required ZIP file is not placed in the correct path, the ETL pipeline will fail or generate incomplete/duplicated outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Required Action\n",
    "\n",
    "Please make sure the following file exists **before running the notebook**:\n",
    "\n",
    "..\\data\\raw\\tennis_data.zip\n",
    "\n",
    "\n",
    "> üìå The path is already configured inside the notebook‚Äôs Python extraction script ‚Äî do **not** change it unless necessary.\n",
    "\n",
    "If the ZIP file has a different name, please rename it or update the code accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 1: Importing the required libraries, defining the paths, and creating the required directories if they do not exist.\n",
    "In this section, we import the libraries and items we need to use them later, and then we define the main paths, such as the main zip file path, the output file directory, and the temp directory, in a relational manner, to be included in the data folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "# Define paths\n",
    "main_zip = r\".\\data\\raw\\tennis_data.zip\"\n",
    "output_dir = r\".\\data\\processed\"\n",
    "temp_dir = r\".\\data\\raw\\temp\"\n",
    "base_path = r\"..\\data\\processed\"\n",
    "clean_path = r\"..\\data\\processed\\clean\"\n",
    "etl_files_path = r\"..\\data\\processed\\clean\"\n",
    "\n",
    "os.makedirs(clean_path, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Part 2: Creating a CSV table generator function from Parquet files\n",
    "In this section, we have created a very useful function that, based on the keyword of the parquet category name that we give it, goes to the defined path of the main zip file and reads the parquets belonging to the specified tables and the data related to the specified columns from the zip files for each day. In addition to all this, we specify that the records of this table should be unique based on the unique data identifier or that this table can have multiple rows for each unique identifier. Our unique identifier is match_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(table_keyword, needed_cols, output_name, dedup_on=\"match_id\"):\n",
    "    \"\"\"\n",
    "    table_keyword: like 'event_' or 'home_team_'\n",
    "    needed_cols: list of needed columns\n",
    "    output_name: name of output CSV file\n",
    "    dedup_on: unique column for deduplication (default is 'match_id')\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(output_dir, output_name)\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    all_dfs = []\n",
    "    row_counter = 0\n",
    "\n",
    "    with zipfile.ZipFile(main_zip, \"r\") as main_zip_ref:\n",
    "        daily_zips = main_zip_ref.namelist()\n",
    "        print(f\"üì¶ Count of daily zips: {len(daily_zips)}\")\n",
    "\n",
    "        for i, daily_zip_name in enumerate(daily_zips, start=1):\n",
    "            print(f\"üîπ ({i}/{len(daily_zips)}) processing {daily_zip_name} ...\")\n",
    "            main_zip_ref.extract(daily_zip_name, temp_dir)\n",
    "            daily_zip_path = os.path.join(temp_dir, daily_zip_name)\n",
    "\n",
    "            with zipfile.ZipFile(daily_zip_path, \"r\") as daily_zip_ref:\n",
    "                parquet_files = [f for f in daily_zip_ref.namelist() if f.endswith(\".parquet\") and table_keyword in f]\n",
    "                for f in parquet_files:\n",
    "                    with daily_zip_ref.open(f) as pf:\n",
    "                        table = pq.read_table(BytesIO(pf.read()))\n",
    "                        df = table.to_pandas()\n",
    "                        df = df[[c for c in needed_cols if c in df.columns]]\n",
    "                        df[\"date_source\"] = daily_zip_name.replace(\".zip\", \"\")\n",
    "                        all_dfs.append(df)\n",
    "                        row_counter += len(df)\n",
    "\n",
    "            os.remove(daily_zip_path)\n",
    "\n",
    "    if all_dfs:\n",
    "        df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Shape: {df_all.shape}\")\n",
    "        if dedup_on and dedup_on in df_all.columns:\n",
    "            df_all = df_all.drop_duplicates(subset=dedup_on)\n",
    "        else:\n",
    "            df_all = df_all.drop_duplicates()\n",
    "        print(f\"üßπ after cleaning duplicated rows: {df_all.shape}\")\n",
    "\n",
    "        df_all.to_csv(csv_path, index=False)\n",
    "        print(f\"üíæ Saved: {csv_path}\")\n",
    "        print(f\"üìä Count of all rows: {len(df_all)}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è There is no file for {table_keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Part 3: Using the above cell function and creating CSVs of the tables required for analysis according to the columns required from them\n",
    "In this part, based on the initial analysis we had of the 17 questions in question and the data they required, we extracted a series of tables from a total of 15 tables and a series of their columns that were needed to analyze and answer the 17 questions we needed. Here, we want to extract them from the original raw zip file and convert them to CSV files so that we can use these files later in analyzing and answering the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_table(\n",
    "    table_keyword=\"event_\",\n",
    "    needed_cols=[\"match_id\", \"first_to_serve\", \"winner_code\", \"default_period_count\", \"start_datetime\", \"match_slug\"],\n",
    "    output_name=\"event.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"home_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"home_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"away_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"away_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"tournament_\",\n",
    "    needed_cols=[\"match_id\", \"tournament_id\", \"tournament_name\", \"ground_type\", \"tennis_points\", \"start_datetime\"],\n",
    "    output_name=\"tournament.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"time_\",\n",
    "    needed_cols=[\"match_id\", \"period_1\", \"period_2\", \"period_3\", \"period_4\", \"period_5\", \"current_period_start_timestamp\"],\n",
    "    output_name=\"time.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"statistics_\",\n",
    "    needed_cols=[\"match_id\", \"statistic_name\", \"home_value\", \"away_value\"],\n",
    "    output_name=\"statistics.csv\",\n",
    "    dedup_on=None  # No deduplication because we have multiple rows per match_id in statistics\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"power_\",\n",
    "    needed_cols=[\"match_id\", \"set_num\", \"game_num\", \"value\", \"break_occurred\"],\n",
    "    output_name=\"power.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in power\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"pbp_\",\n",
    "    needed_cols=[\"match_id\", \"set_id\", \"game_id\", \"point_id\", \"home_point\", \"away_point\", \"home_score\"],\n",
    "    output_name=\"pbp.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in pbp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "##  Part 4: Data Cleaning Stage\n",
    "#### In this section, we will clean the extracted CSV files created in the previous section.\n",
    "# \n",
    "### **Goal:**  \n",
    "- Remove duplicate rows  \n",
    " - Handle missing values (`NaN`)  \n",
    " - Standardize data types  \n",
    " The cleaned outputs will be stored in `../data/clean` for the next normalization phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "###  Cleaning: Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(os.path.join(base_path, \"event.csv\"))\n",
    "df_event.drop_duplicates(inplace=True)\n",
    "\n",
    "for col in df_event.columns:\n",
    "    if df_event[col].dtype == 'object':\n",
    "        df_event[col] = df_event[col].fillna(\"Unknown\")\n",
    "    else:\n",
    "        df_event[col] = df_event[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_event.columns:\n",
    "    df_event[\"match_id\"] = df_event[\"match_id\"].astype(str)\n",
    "\n",
    "df_event.to_csv(os.path.join(clean_path, \"event_clean.csv\"), index=False)\n",
    "print(\"‚úÖ event_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "###  Cleaning: Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(base_path, \"home_team.csv\"))\n",
    "df_home = df_home.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(str)\n",
    "\n",
    "df_home.to_csv(os.path.join(clean_path, \"home_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ home_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "###  Cleaning: Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_away = pd.read_csv(os.path.join(base_path, \"away_team.csv\"))\n",
    "df_away = df_away.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(str)\n",
    "\n",
    "df_away.to_csv(os.path.join(clean_path, \"away_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ away_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Cleaning: Tournamet Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tournament = pd.read_csv(os.path.join(base_path, \"tournament.csv\"))\n",
    "df_tournament = df_tournament.drop_duplicates()\n",
    "\n",
    "mask = df_tournament['ground_type'].isnull() | (df_tournament['ground_type'].str.strip() == '')\n",
    "df_tournament.loc[mask, 'ground_type'] = 'Unknown'\n",
    "\n",
    "df_tournament.to_csv(os.path.join(clean_path, \"tournament_clean.csv\"), index=False)\n",
    "print(\"‚úÖ tournament_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Cleaning: Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistics = pd.read_csv(os.path.join(base_path, \"statistics.csv\"))\n",
    "df_statistics = df_statistics.drop_duplicates()\n",
    "\n",
    "# This data dosent have any nulls to clean however we display the count of nulls for verification\n",
    "display(df_statistics.isnull().sum())\n",
    "\n",
    "if \"match_id\" in df_statistics.columns:\n",
    "    df_statistics[\"match_id\"] = df_statistics[\"match_id\"].astype(str)\n",
    "\n",
    "df_statistics.to_csv(os.path.join(clean_path, \"statistics_clean.csv\"), index=False)\n",
    "print(\"‚úÖ statistics_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Cleaning: Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.read_csv(os.path.join(base_path, \"time.csv\"))\n",
    "df_time = df_time.drop_duplicates()\n",
    "\n",
    "df_time.drop(columns=[\"period_4\", \"period_5\"], inplace=True) # because all tennis matches are best of 3 sets\n",
    "\n",
    "if \"match_id\" in df_time.columns:\n",
    "    df_time[\"match_id\"] = df_time[\"match_id\"].astype(int)\n",
    "\n",
    "df_time.to_csv(os.path.join(clean_path, \"time_clean.csv\"), index=False)\n",
    "print(\"‚úÖ time_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Cleaning: Point By Point Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pbp = pd.read_csv(os.path.join(base_path, \"pbp.csv\"))\n",
    "df_pbp = df_pbp.drop_duplicates()\n",
    "\n",
    "df_pbp['home_point'] = df_pbp['home_point'].replace('A', 1).astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].replace('A', 1).astype(int)\n",
    "\n",
    "# This data dosent have any nulls to clean however we display the count of nulls for verification\n",
    "display(df_pbp.isnull().sum())\n",
    "\n",
    "if \"match_id\" in df_pbp.columns:\n",
    "    df_pbp[\"match_id\"] = df_pbp[\"match_id\"].astype(str)\n",
    "\n",
    "\n",
    "df_pbp.to_csv(os.path.join(clean_path, \"pbp_clean.csv\"), index=False)\n",
    "print(\"‚úÖ pbp_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Cleaning: Power Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power = pd.read_csv(os.path.join(base_path, \"power.csv\"))\n",
    "df_power = df_power.drop_duplicates()\n",
    "\n",
    "# No nulls to clean, just save the cleaned file and other column is clean as you can see here\n",
    "display(\"Sum of nulls:\", df_power.isnull().sum())\n",
    "display(\"Sum of invalid game_num entries:\", df_power['match_id'][df_power['game_num'] < 1].sum())\n",
    "display(\"DataFrame dtypes for verification correctness of data:\", df_power.dtypes)\n",
    "\n",
    "df_power.to_csv(os.path.join(clean_path, \"power_clean.csv\"), index=False)\n",
    "print(\"‚úÖ power_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Part 5: Normalization Stage\n",
    "#### Now that we have clean CSVs, in this part we will:\n",
    "#\n",
    " - Convert data types (e.g., timestamps to datetime)  \n",
    " - Standardize text (e.g., capitalization, spacing)  \n",
    " - Fill remaining missing values intelligently (using mean, median, or mode)  \n",
    " The normalized final datasets will be saved in `../data/processed/clean` as `_final.csv` files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"event_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"event_final.csv\")\n",
    "\n",
    "df_event = pd.read_csv(input_path)\n",
    "\n",
    "df_event[\"match_id\"] = df_event[\"match_id\"].astype(int)\n",
    "df_event[\"default_period_count\"] = df_event[\"default_period_count\"].astype(int)\n",
    "df_event[\"date_source\"] = df_event[\"date_source\"].astype(int)\n",
    "\n",
    "if np.issubdtype(df_event[\"start_datetime\"].dtype, np.number):\n",
    "    df_event[\"start_datetime\"] = pd.to_datetime(df_event[\"start_datetime\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "df_event[\"winner_code\"] = df_event[\"winner_code\"].fillna(df_event[\"winner_code\"].mode()[0])\n",
    "df_event[\"first_to_serve\"] = df_event[\"first_to_serve\"].fillna(df_event[\"first_to_serve\"].mode()[0])\n",
    "\n",
    "df_event.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ event_final.csv created successfully!\")\n",
    "print(df_event.info())\n",
    "print(df_event.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"home_team_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"home_team_final.csv\")\n",
    "\n",
    "df_home = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = pd.to_numeric(df_home[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_home.columns:\n",
    "    df_home[\"gender\"] = df_home[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_home.columns:\n",
    "    df_home[\"plays\"] = df_home[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_home.columns:\n",
    "    df_home[\"height\"] = df_home[\"height\"].fillna(df_home[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_home.columns:\n",
    "    df_home[\"weight\"] = df_home[\"weight\"].fillna(df_home[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_home.columns:\n",
    "    df_home[\"current_rank\"] = df_home[\"current_rank\"].fillna(df_home[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_home.columns:\n",
    "        mode_val = df_home[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_home[col] = df_home[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(int)\n",
    "\n",
    "df_home.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ home_team_final.csv created successfully!\")\n",
    "print(df_home.info())\n",
    "print(df_home.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"away_team_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"away_team_final.csv\")\n",
    "\n",
    "df_away = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = pd.to_numeric(df_away[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_away.columns:\n",
    "    df_away[\"gender\"] = df_away[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_away.columns:\n",
    "    df_away[\"plays\"] = df_away[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_away.columns:\n",
    "    df_away[\"height\"] = df_away[\"height\"].fillna(df_away[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_away.columns:\n",
    "    df_away[\"weight\"] = df_away[\"weight\"].fillna(df_away[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_away.columns:\n",
    "    df_away[\"current_rank\"] = df_away[\"current_rank\"].fillna(df_away[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_away.columns:\n",
    "        mode_val = df_away[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_away[col] = df_away[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(int)\n",
    "\n",
    "df_away.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ away_team_final.csv created successfully!\")\n",
    "print(df_away.info())\n",
    "print(df_away.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Tournament Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"tournament_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"tournament_final.csv\")\n",
    "\n",
    "df_tournament = pd.read_csv(input_path)\n",
    "\n",
    "df_tournament['match_id'] = df_tournament['match_id'].astype(int)\n",
    "\n",
    "df_tournament.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ tournament_final.csv created successfully!\")\n",
    "print(df_tournament.info())\n",
    "print(df_tournament.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"statistics_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"statistics_final.csv\")\n",
    "\n",
    "df_statistics = pd.read_csv(input_path)\n",
    "\n",
    "df_statistics['date_source'] = pd.to_datetime(df_statistics['date_source'], format='%Y%m%d')\n",
    "df_statistics['home_value'] = pd.to_numeric(df_statistics['home_value'], errors='coerce')\n",
    "df_statistics['away_value'] = pd.to_numeric(df_statistics['away_value'], errors='coerce')\n",
    "df_statistics['statistic_name'] = df_statistics['statistic_name'].astype(str).str.replace(\" \", \"_\").str.lower()\n",
    "df_statistics['match_id'] = df_statistics['match_id'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "df_statistics.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ statistics_final.csv created successfully!\")\n",
    "print(df_statistics.info())\n",
    "print(df_statistics.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"time_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"time_final.csv\")\n",
    "\n",
    "df_time = pd.read_csv(input_path)\n",
    "\n",
    "periods = [\"period_1\", \"period_2\", \"period_3\"]\n",
    "\n",
    "df_time[\"match_id\"] = df_time[\"match_id\"].astype(int)\n",
    "df_time['date_source'] = pd.to_datetime(df_time['date_source'], format='%Y%m%d')\n",
    "df_time['current_period_start_timestamp'] = pd.to_datetime(df_time['current_period_start_timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "df_time['match_id'] = df_time['match_id'].astype(int)\n",
    "\n",
    "MS_TRESHOLD = 100_000  # 100,000 milliseconds = 100 seconds\n",
    "\n",
    "for period in periods:\n",
    "    df_time[period] = pd.to_numeric(df_time[period], errors='coerce').abs()\n",
    "    mask = df_time[period] > MS_TRESHOLD\n",
    "    df_time.loc[mask, period] = df_time.loc[mask, period] / 1000 # convert milliseconds to seconds\n",
    "\n",
    "\n",
    "df_time.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ time_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Point By Point Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"pbp_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"pbp_final.csv\")\n",
    "\n",
    "df_pbp = pd.read_csv(input_path)\n",
    "\n",
    "df_pbp['date_source'] = pd.to_datetime(df_pbp['date_source'], format='%Y%m%d')\n",
    "df_pbp['match_id'] = df_pbp['match_id'].astype(int)\n",
    "df_pbp['home_point'] = df_pbp['home_point'].astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].astype(int)\n",
    "df_pbp['home_score'] = df_pbp['home_score'].astype(int)\n",
    "df_pbp['set_id'] = df_pbp['set_id'].astype(int)\n",
    "df_pbp['game_id'] = df_pbp['game_id'].astype(int)\n",
    "df_pbp['point_id'] = df_pbp['point_id'].astype(int)\n",
    "\n",
    "df_pbp.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ pbp_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Power Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"pbp_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"pbp_final.csv\")\n",
    "\n",
    "df_pbp = pd.read_csv(input_path)\n",
    "\n",
    "df_pbp['date_source'] = pd.to_datetime(df_pbp['date_source'], format='%Y%m%d')\n",
    "df_pbp['match_id'] = df_pbp['match_id'].astype(int)\n",
    "df_pbp['home_point'] = df_pbp['home_point'].astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].astype(int)\n",
    "df_pbp['set_id'] = df_pbp['set_id'].astype(int)\n",
    "df_pbp['game_id'] = df_pbp['game_id'].astype(int)\n",
    "df_pbp['point_id'] = df_pbp['point_id'].astype(int)\n",
    "\n",
    "df_pbp.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ pbp_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Question 1 ‚Äî How many tennis players are included in the dataset?\n",
    "\n",
    "To find the number of unique tennis players, we combine the home and away team tables, remove duplicate players based on `player_id`, and count how many unique players remain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "# Load cleaned & normalized player data\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# Combine home and away players\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# Keep only unique players\n",
    "unique_players = players.drop_duplicates(subset=\"player_id\")\n",
    "\n",
    "# Count unique players\n",
    "total_unique_players = unique_players.shape[0]\n",
    "\n",
    "print(\"total unique players =\",total_unique_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "###  Question 2 ‚Äî What is the average height of the players?\n",
    "The goal of this question is to calculate the average height of the tennis players in the dataset.\n",
    "To do this, we first need to create a unique players table so that players who are duplicates in home and away are not counted again.\n",
    "Then we correct invalid heights (such as 0 or NaN) and calculate the true average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "# load data\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# combine & remove duplicate players\n",
    "players = pd.concat([df_home , df_away], ignore_index=True)\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\").copy()\n",
    "\n",
    "# replace zeros with NaN\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].replace(0, np.nan)\n",
    "\n",
    "# fill missing heights with mean\n",
    "mean_height = players_unique[\"height\"].mean(skipna=True)\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].fillna(mean_height)\n",
    "\n",
    "# Calculate average height\n",
    "average_height = players_unique[\"height\"].mean()\n",
    "print(\"Average height =\" , average_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The histogram + KDE curve helps us observe:  \n",
    "- The central height tendency (mean around ~182 cm)  \n",
    "- Spread of heights  \n",
    "- Possible outliers  \n",
    "- Whether the distribution is normal or skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Player Height Distribution\")\n",
    "sns.histplot(players_unique[\"height\"], kde=True, bins=30)\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Insight\n",
    "The average tennis player height is around **182 cm**.  \n",
    "The distribution appears slightly right-skewed, meaning a small number of players are significantly taller than the average.\n",
    "\n",
    "Most players fall between **175‚Äì190 cm**, which aligns with typical professional tennis standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Question 10 ‚Äî Correlation Between Player Height and Ranking\n",
    "Interpretation\n",
    "\n",
    "We want to check whether taller players tend to have higher or lower rankings.\n",
    "We combine home & away players, remove duplicates, clean invalid values, and compute Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "# Load datasets\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# Merge home + away\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# Keep unique players\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\")\n",
    "\n",
    "# Replace invalid zeros with NaN\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].replace(0, np.nan)\n",
    "players_unique.loc[:, \"current_rank\"] = players_unique[\"current_rank\"].replace(0, np.nan)\n",
    "\n",
    "# Drop rows with missing required values\n",
    "clean_players = players_unique.dropna(subset=[\"height\", \"current_rank\"])\n",
    "\n",
    "# Compute correlation\n",
    "correlation = clean_players[\"height\"].corr(clean_players[\"current_rank\"], method=\"pearson\")\n",
    "\n",
    "print(\"correlation =\" , correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Correlation = 0.10355\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "There is no meaningful correlation between player height and ranking.\n",
    "Height does not significantly influence global ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(\n",
    "    x=\"height\",\n",
    "    y=\"current_rank\",\n",
    "    data=clean_players,\n",
    "    scatter_kws={\"alpha\":0.4},\n",
    "    line_kws={\"color\":\"red\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Height vs Ranking\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Ranking (Lower is Better)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "###  Insight\n",
    "The correlation coefficient was approximately **0.10**, indicating a **very weak positive correlation**.\n",
    "\n",
    "This means **taller players tend to have slightly worse rankings**, but the effect is extremely small.  \n",
    "\n",
    "Height does NOT strongly predict performance or ranking in professional tennis.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Question 11 ‚Äî What is the average duration of matches?\n",
    "\n",
    "To calculate the average match duration, we used the `time_final.csv` dataset, which contains\n",
    "the duration of each period inside a tennis match:\n",
    "\n",
    "- `period_1`\n",
    "- `period_2`\n",
    "- `period_3`\n",
    "\n",
    "These periods contain the **duration in seconds**.\n",
    "\n",
    "#### **Steps**\n",
    "1. Replace NaN values in period columns with 0.  \n",
    "2. Compute total match duration:  `duration_seconds = period_1 + period_2 + period_3`  \n",
    "3. Keep only matches where duration > 0.  \n",
    "4. Compute the mean duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "df_time = pd.read_csv(os.path.join(base, \"time_final.csv\"))\n",
    "\n",
    "# Replace NaN periods with 0\n",
    "for col in [\"period_1\", \"period_2\", \"period_3\"]:\n",
    "    df_time[col] = df_time[col].fillna(0)\n",
    "\n",
    "# Compute duration in seconds\n",
    "df_time.loc[:, \"duration_seconds\"] = df_time[\"period_1\"] + df_time[\"period_2\"] + df_time[\"period_3\"]\n",
    "\n",
    "# Filter out zeros (invalid matches)\n",
    "df_valid = df_time[df_time[\"duration_seconds\"] > 0].copy()\n",
    "df_valid[\"duration_minutes\"] = df_valid[\"duration_seconds\"] / 60\n",
    "\n",
    "# Average duration\n",
    "avg_sec = df_valid[\"duration_seconds\"].mean()\n",
    "avg_minutes = avg_sec / 60\n",
    "avg_hours = avg_minutes / 60\n",
    "\n",
    "print(\"Average duration (seconds)=\", avg_sec)\n",
    "print(\"Average duration (minutes)=\", avg_minutes)\n",
    "print(\"Average duration (hours)=\", avg_hours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- **Average duration (seconds):** 6705.87  \n",
    "- **Average duration (minutes):** 111.76  \n",
    "- **Average duration (hours):** 1.86  \n",
    "\n",
    "#### **Average tennis match duration ‚âà 1 hour and 52 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Distribution of Match Duration\")\n",
    "sns.histplot(df_valid[\"duration_minutes\"], kde=True, bins=40)\n",
    "plt.xlabel(\"Duration (minutes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "###  Insight\n",
    "\n",
    "The distribution shows:  \n",
    "- Most matches are between **60 and 130 minutes**  \n",
    "- A small number of matches last more than **180 minutes**  \n",
    "- Very short or extremely long matches are rare  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Question 12 - What is the average number of games per set in men's matches compared to women's matches?\n",
    "\n",
    "To calculate the average number of games per set for men's and women's matches, we used three main datasets: `home_team_final.csv`, `away_team_final.csv`, and `pbp_final.csv`.\n",
    "\n",
    "The `gender` column in the `home_team_final.csv` and `away_team_final.csv` datasets contains information about the **gender** of the players, and in the `pbp_final.csv` dataset we can also have `information about each game` to count it into women's and men's matches.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading datasets and storing them in code as dataframes\n",
    "2. Create a `gender` dataframe that contains information about the gender and matches played by home and away players.\n",
    "3. Obtaining the gender of each match using player grouping and getting the mode of the players for each match with the code `df_gender = df_gender.groupby(\"match_id\")['gender'].apply(lambda x: x.mode()[0]).reset_index()`\n",
    "4. Counting games per match and storing them in `df_games`\n",
    "5. Merging the `df_gender` and `df_games` dataframes for the final calculation\n",
    "6. Obtain the final result by grouping `df_merged` by gender and averaging for each gender (ignoring data where the `gender` column is unknown)\n",
    "7. Obtaining the ratio of gender-unknown data to total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\"))\n",
    "df_pbp = pd.read_csv(os.path.join(etl_files_path, \"pbp_final.csv\"))\n",
    "\n",
    "df_gender = pd.concat(\n",
    "    [df_home[['match_id', 'gender']],\n",
    "     df_away[['match_id', 'gender']]],\n",
    "    ignore_index=True)\n",
    "\n",
    "df_gender = df_gender.groupby(\"match_id\")['gender'].apply(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "df_games = df_pbp.groupby(['match_id', 'set_id',])['game_id'].nunique().reset_index(name='games_in_set')\n",
    "\n",
    "df_merged = df_games.merge(df_gender, on='match_id', how='inner')\n",
    "\n",
    "result = df_merged.groupby('gender')['games_in_set'].mean().drop('Unknown')\n",
    "\n",
    "stat = df_merged.groupby('gender')['games_in_set']\n",
    "\n",
    "ratio_of_unknown = (df_merged['gender'] == 'Unknown').sum() / len(df_merged) * 100\n",
    "\n",
    "print(result)\n",
    "print(ratio_of_unknown)\n",
    "print('minimum games in a match', stat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- The average number of games per set in men's matches is approximately equal to **9.27**\n",
    "- The average number of games per set in women's matches is approximately equal to **8.90**\n",
    "- The highest number of games in a match for both women and men was **13**.\n",
    "\n",
    "#### The proportion of players whose gender was unknown to the total dataset is approximately equal to **5.1%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.barplot(\n",
    "    y=['Male', 'Female'],\n",
    "    x=result.values,\n",
    "    ax=ax,\n",
    "    color='skyblue',\n",
    "    hue=result.index,\n",
    "    edgecolor = 'gray',\n",
    "    palette=['skyblue', 'skyblue'],\n",
    "    alpha = 0.9\n",
    ")\n",
    "ax.set_title('Average Number of Games per Match by Gender', fontsize=16, loc='left')\n",
    "ax.set_ylabel('Gender', fontsize=14, loc='bottom', color='gray')\n",
    "ax.set_xlabel('Games per Match', fontsize=14, loc=\"left\", color='gray')\n",
    "ax.tick_params(axis='both', which='major', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "ax.set_xticks(np.arange(0, 10, 1))\n",
    "ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- The average number of games in a match between women and men is not much different **(around 0.3 games per match)**.\n",
    "- About **5.1%** of players had their gender unknown in the dataset and were not included in this statistic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
