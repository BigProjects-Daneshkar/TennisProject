{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Reading Section:\n",
    "##### In this section, we will use the following codes, which will be explained below, to read the data we need from the main reference file and convert it to CSV format so that we can use it in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è IMPORTANT ‚Äî READ BEFORE RUNNING\n",
    "\n",
    "This notebook expects the raw dataset to be available **before execution**.  \n",
    "If the required ZIP file is not placed in the correct path, the ETL pipeline will fail or generate incomplete/duplicated outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Required Action\n",
    "\n",
    "Please make sure the following file exists **before running the notebook**:\n",
    "\n",
    "..\\data\\raw\\tennis_data.zip\n",
    "\n",
    "\n",
    "> üìå The path is already configured inside the notebook‚Äôs Python extraction script ‚Äî do **not** change it unless necessary.\n",
    "\n",
    "If the ZIP file has a different name, please rename it or update the code accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 1: Importing the required libraries, defining the paths, and creating the required directories if they do not exist.\n",
    "In this section, we import the libraries and items we need to use them later, and then we define the main paths, such as the main zip file path, the output file directory, and the temp directory, in a relational manner, to be included in the data folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "# Define paths\n",
    "main_zip = r\".\\data\\raw\\tennis_data.zip\"\n",
    "output_dir = r\".\\data\\processed\"\n",
    "temp_dir = r\".\\data\\raw\\temp\"\n",
    "base_path = r\"..\\data\\processed\"\n",
    "clean_path = r\"..\\data\\processed\\clean\"\n",
    "etl_files_path = r\"..\\data\\processed\\clean\"\n",
    "\n",
    "os.makedirs(clean_path, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Part 2: Creating a CSV table generator function from Parquet files\n",
    "In this section, we have created a very useful function that, based on the keyword of the parquet category name that we give it, goes to the defined path of the main zip file and reads the parquets belonging to the specified tables and the data related to the specified columns from the zip files for each day. In addition to all this, we specify that the records of this table should be unique based on the unique data identifier or that this table can have multiple rows for each unique identifier. Our unique identifier is match_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(table_keyword, needed_cols, output_name, dedup_on=\"match_id\"):\n",
    "    \"\"\"\n",
    "    table_keyword: like 'event_' or 'home_team_'\n",
    "    needed_cols: list of needed columns\n",
    "    output_name: name of output CSV file\n",
    "    dedup_on: unique column for deduplication (default is 'match_id')\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(output_dir, output_name)\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    all_dfs = []\n",
    "    row_counter = 0\n",
    "\n",
    "    with zipfile.ZipFile(main_zip, \"r\") as main_zip_ref:\n",
    "        daily_zips = main_zip_ref.namelist()\n",
    "        print(f\"üì¶ Count of daily zips: {len(daily_zips)}\")\n",
    "\n",
    "        for i, daily_zip_name in enumerate(daily_zips, start=1):\n",
    "            print(f\"üîπ ({i}/{len(daily_zips)}) processing {daily_zip_name} ...\")\n",
    "            main_zip_ref.extract(daily_zip_name, temp_dir)\n",
    "            daily_zip_path = os.path.join(temp_dir, daily_zip_name)\n",
    "\n",
    "            with zipfile.ZipFile(daily_zip_path, \"r\") as daily_zip_ref:\n",
    "                parquet_files = [f for f in daily_zip_ref.namelist() if f.endswith(\".parquet\") and table_keyword in f]\n",
    "                for f in parquet_files:\n",
    "                    with daily_zip_ref.open(f) as pf:\n",
    "                        table = pq.read_table(BytesIO(pf.read()))\n",
    "                        df = table.to_pandas()\n",
    "                        df = df[[c for c in needed_cols if c in df.columns]]\n",
    "                        df[\"date_source\"] = daily_zip_name.replace(\".zip\", \"\")\n",
    "                        all_dfs.append(df)\n",
    "                        row_counter += len(df)\n",
    "\n",
    "            os.remove(daily_zip_path)\n",
    "\n",
    "    if all_dfs:\n",
    "        df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Shape: {df_all.shape}\")\n",
    "        if dedup_on and dedup_on in df_all.columns:\n",
    "            df_all = df_all.drop_duplicates(subset=dedup_on)\n",
    "        else:\n",
    "            df_all = df_all.drop_duplicates()\n",
    "        print(f\"üßπ after cleaning duplicated rows: {df_all.shape}\")\n",
    "\n",
    "        df_all.to_csv(csv_path, index=False)\n",
    "        print(f\"üíæ Saved: {csv_path}\")\n",
    "        print(f\"üìä Count of all rows: {len(df_all)}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è There is no file for {table_keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Part 3: Using the above cell function and creating CSVs of the tables required for analysis according to the columns required from them\n",
    "In this part, based on the initial analysis we had of the 17 questions in question and the data they required, we extracted a series of tables from a total of 15 tables and a series of their columns that were needed to analyze and answer the 17 questions we needed. Here, we want to extract them from the original raw zip file and convert them to CSV files so that we can use these files later in analyzing and answering the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_table(\n",
    "    table_keyword=\"event_\",\n",
    "    needed_cols=[\"match_id\", \"first_to_serve\", \"winner_code\", \"default_period_count\", \"start_datetime\", \"match_slug\"],\n",
    "    output_name=\"event.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"home_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"home_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"away_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"away_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"tournament_\",\n",
    "    needed_cols=[\"match_id\", \"tournament_id\", \"tournament_name\", \"ground_type\", \"tennis_points\", \"start_datetime\"],\n",
    "    output_name=\"tournament.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"time_\",\n",
    "    needed_cols=[\"match_id\", \"period_1\", \"period_2\", \"period_3\", \"period_4\", \"period_5\", \"current_period_start_timestamp\"],\n",
    "    output_name=\"time.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"statistics_\",\n",
    "    needed_cols=[\"match_id\", \"statistic_name\", \"home_value\", \"away_value\"],\n",
    "    output_name=\"statistics.csv\",\n",
    "    dedup_on=None  # No deduplication because we have multiple rows per match_id in statistics\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"power_\",\n",
    "    needed_cols=[\"match_id\", \"set_num\", \"game_num\", \"value\", \"break_occurred\"],\n",
    "    output_name=\"power.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in power\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"pbp_\",\n",
    "    needed_cols=[\"match_id\", \"set_id\", \"game_id\", \"point_id\", \"home_point\", \"away_point\", \"home_score\"],\n",
    "    output_name=\"pbp.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in pbp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "##  Part 4: Data Cleaning Stage\n",
    "#### In this section, we will clean the extracted CSV files created in the previous section.\n",
    "# \n",
    "### **Goal:**  \n",
    "- Remove duplicate rows  \n",
    " - Handle missing values (`NaN`)  \n",
    " - Standardize data types  \n",
    " The cleaned outputs will be stored in `../data/clean` for the next normalization phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "###  Cleaning: Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(os.path.join(base_path, \"event.csv\"))\n",
    "df_event.drop_duplicates(inplace=True)\n",
    "\n",
    "for col in df_event.columns:\n",
    "    if df_event[col].dtype == 'object':\n",
    "        df_event[col] = df_event[col].fillna(\"Unknown\")\n",
    "    else:\n",
    "        df_event[col] = df_event[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_event.columns:\n",
    "    df_event[\"match_id\"] = df_event[\"match_id\"].astype(str)\n",
    "\n",
    "df_event.to_csv(os.path.join(clean_path, \"event_clean.csv\"), index=False)\n",
    "print(\"‚úÖ event_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "###  Cleaning: Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(base_path, \"home_team.csv\"))\n",
    "df_home = df_home.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(str)\n",
    "\n",
    "df_home.to_csv(os.path.join(clean_path, \"home_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ home_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "###  Cleaning: Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_away = pd.read_csv(os.path.join(base_path, \"away_team.csv\"))\n",
    "df_away = df_away.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(str)\n",
    "\n",
    "df_away.to_csv(os.path.join(clean_path, \"away_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ away_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Cleaning: Tournamet Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tournament = pd.read_csv(os.path.join(base_path, \"tournament.csv\"))\n",
    "df_tournament = df_tournament.drop_duplicates()\n",
    "\n",
    "mask = df_tournament['ground_type'].isnull() | (df_tournament['ground_type'].str.strip() == '')\n",
    "df_tournament.loc[mask, 'ground_type'] = 'Unknown'\n",
    "\n",
    "df_tournament.to_csv(os.path.join(clean_path, \"tournament_clean.csv\"), index=False)\n",
    "print(\"‚úÖ tournament_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Cleaning: Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistics = pd.read_csv(os.path.join(base_path, \"statistics.csv\"))\n",
    "df_statistics = df_statistics.drop_duplicates()\n",
    "\n",
    "# This data dosent have any nulls to clean however we display the count of nulls for verification\n",
    "display(df_statistics.isnull().sum())\n",
    "\n",
    "if \"match_id\" in df_statistics.columns:\n",
    "    df_statistics[\"match_id\"] = df_statistics[\"match_id\"].astype(str)\n",
    "\n",
    "df_statistics.to_csv(os.path.join(clean_path, \"statistics_clean.csv\"), index=False)\n",
    "print(\"‚úÖ statistics_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Cleaning: Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.read_csv(os.path.join(base_path, \"time.csv\"))\n",
    "df_time = df_time.drop_duplicates()\n",
    "\n",
    "df_time.drop(columns=[\"period_4\", \"period_5\"], inplace=True) # because all tennis matches are best of 3 sets\n",
    "\n",
    "if \"match_id\" in df_time.columns:\n",
    "    df_time[\"match_id\"] = df_time[\"match_id\"].astype(int)\n",
    "\n",
    "df_time.to_csv(os.path.join(clean_path, \"time_clean.csv\"), index=False)\n",
    "print(\"‚úÖ time_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Cleaning: Point By Point Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pbp = pd.read_csv(os.path.join(base_path, \"pbp.csv\"))\n",
    "df_pbp = df_pbp.drop_duplicates()\n",
    "\n",
    "df_pbp['home_point'] = df_pbp['home_point'].replace('A', 1).astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].replace('A', 1).astype(int)\n",
    "\n",
    "# This data dosent have any nulls to clean however we display the count of nulls for verification\n",
    "display(df_pbp.isnull().sum())\n",
    "\n",
    "if \"match_id\" in df_pbp.columns:\n",
    "    df_pbp[\"match_id\"] = df_pbp[\"match_id\"].astype(str)\n",
    "\n",
    "\n",
    "df_pbp.to_csv(os.path.join(clean_path, \"pbp_clean.csv\"), index=False)\n",
    "print(\"‚úÖ pbp_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Cleaning: Power Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power = pd.read_csv(os.path.join(base_path, \"power.csv\"))\n",
    "df_power = df_power.drop_duplicates()\n",
    "\n",
    "# No nulls to clean, just save the cleaned file and other column is clean as you can see here\n",
    "display(\"Sum of nulls:\", df_power.isnull().sum())\n",
    "display(\"Sum of invalid game_num entries:\", df_power['match_id'][df_power['game_num'] < 1].sum())\n",
    "display(\"DataFrame dtypes for verification correctness of data:\", df_power.dtypes)\n",
    "\n",
    "df_power.to_csv(os.path.join(clean_path, \"power_clean.csv\"), index=False)\n",
    "print(\"‚úÖ power_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Part 5: Normalization Stage\n",
    "#### Now that we have clean CSVs, in this part we will:\n",
    "#\n",
    " - Convert data types (e.g., timestamps to datetime)  \n",
    " - Standardize text (e.g., capitalization, spacing)  \n",
    " - Fill remaining missing values intelligently (using mean, median, or mode)  \n",
    " The normalized final datasets will be saved in `../data/processed/clean` as `_final.csv` files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"event_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"event_final.csv\")\n",
    "\n",
    "df_event = pd.read_csv(input_path)\n",
    "\n",
    "df_event[\"match_id\"] = df_event[\"match_id\"].astype(int)\n",
    "df_event[\"default_period_count\"] = df_event[\"default_period_count\"].astype(int)\n",
    "df_event[\"date_source\"] = df_event[\"date_source\"].astype(int)\n",
    "\n",
    "if np.issubdtype(df_event[\"start_datetime\"].dtype, np.number):\n",
    "    df_event[\"start_datetime\"] = pd.to_datetime(df_event[\"start_datetime\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "df_event[\"winner_code\"] = df_event[\"winner_code\"].fillna(df_event[\"winner_code\"].mode()[0])\n",
    "df_event[\"first_to_serve\"] = df_event[\"first_to_serve\"].fillna(df_event[\"first_to_serve\"].mode()[0])\n",
    "\n",
    "df_event.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ event_final.csv created successfully!\")\n",
    "print(df_event.info())\n",
    "print(df_event.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"home_team_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"home_team_final.csv\")\n",
    "\n",
    "df_home = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = pd.to_numeric(df_home[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_home.columns:\n",
    "    df_home[\"gender\"] = df_home[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_home.columns:\n",
    "    df_home[\"plays\"] = df_home[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_home.columns:\n",
    "    df_home[\"height\"] = df_home[\"height\"].fillna(df_home[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_home.columns:\n",
    "    df_home[\"weight\"] = df_home[\"weight\"].fillna(df_home[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_home.columns:\n",
    "    df_home[\"current_rank\"] = df_home[\"current_rank\"].fillna(df_home[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_home.columns:\n",
    "        mode_val = df_home[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_home[col] = df_home[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(int)\n",
    "\n",
    "df_home.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ home_team_final.csv created successfully!\")\n",
    "print(df_home.info())\n",
    "print(df_home.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"away_team_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"away_team_final.csv\")\n",
    "\n",
    "df_away = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = pd.to_numeric(df_away[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_away.columns:\n",
    "    df_away[\"gender\"] = df_away[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_away.columns:\n",
    "    df_away[\"plays\"] = df_away[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_away.columns:\n",
    "    df_away[\"height\"] = df_away[\"height\"].fillna(df_away[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_away.columns:\n",
    "    df_away[\"weight\"] = df_away[\"weight\"].fillna(df_away[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_away.columns:\n",
    "    df_away[\"current_rank\"] = df_away[\"current_rank\"].fillna(df_away[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_away.columns:\n",
    "        mode_val = df_away[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_away[col] = df_away[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(int)\n",
    "\n",
    "df_away.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ away_team_final.csv created successfully!\")\n",
    "print(df_away.info())\n",
    "print(df_away.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Tournament Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"tournament_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"tournament_final.csv\")\n",
    "\n",
    "df_tournament = pd.read_csv(input_path)\n",
    "\n",
    "df_tournament['match_id'] = df_tournament['match_id'].astype(int)\n",
    "\n",
    "df_tournament.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ tournament_final.csv created successfully!\")\n",
    "print(df_tournament.info())\n",
    "print(df_tournament.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"statistics_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"statistics_final.csv\")\n",
    "\n",
    "df_statistics = pd.read_csv(input_path)\n",
    "\n",
    "df_statistics['date_source'] = pd.to_datetime(df_statistics['date_source'], format='%Y%m%d')\n",
    "df_statistics['home_value'] = pd.to_numeric(df_statistics['home_value'], errors='coerce')\n",
    "df_statistics['away_value'] = pd.to_numeric(df_statistics['away_value'], errors='coerce')\n",
    "df_statistics['statistic_name'] = df_statistics['statistic_name'].astype(str).str.replace(\" \", \"_\").str.lower()\n",
    "df_statistics['match_id'] = df_statistics['match_id'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "df_statistics.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ statistics_final.csv created successfully!\")\n",
    "print(df_statistics.info())\n",
    "print(df_statistics.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"time_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"time_final.csv\")\n",
    "\n",
    "df_time = pd.read_csv(input_path)\n",
    "\n",
    "periods = [\"period_1\", \"period_2\", \"period_3\"]\n",
    "\n",
    "df_time[\"match_id\"] = df_time[\"match_id\"].astype(int)\n",
    "df_time['date_source'] = pd.to_datetime(df_time['date_source'], format='%Y%m%d')\n",
    "df_time['current_period_start_timestamp'] = pd.to_datetime(df_time['current_period_start_timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "df_time['match_id'] = df_time['match_id'].astype(int)\n",
    "\n",
    "MS_TRESHOLD = 100_000  # 100,000 milliseconds = 100 seconds\n",
    "\n",
    "for period in periods:\n",
    "    df_time[period] = pd.to_numeric(df_time[period], errors='coerce').abs()\n",
    "    mask = df_time[period] > MS_TRESHOLD\n",
    "    df_time.loc[mask, period] = df_time.loc[mask, period] / 1000 # convert milliseconds to seconds\n",
    "\n",
    "\n",
    "df_time.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ time_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Point By Point Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"pbp_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"pbp_final.csv\")\n",
    "\n",
    "df_pbp = pd.read_csv(input_path)\n",
    "\n",
    "df_pbp['date_source'] = pd.to_datetime(df_pbp['date_source'], format='%Y%m%d')\n",
    "df_pbp['match_id'] = df_pbp['match_id'].astype(int)\n",
    "df_pbp['home_point'] = df_pbp['home_point'].astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].astype(int)\n",
    "df_pbp['home_score'] = df_pbp['home_score'].astype(int)\n",
    "df_pbp['set_id'] = df_pbp['set_id'].astype(int)\n",
    "df_pbp['game_id'] = df_pbp['game_id'].astype(int)\n",
    "df_pbp['point_id'] = df_pbp['point_id'].astype(int)\n",
    "\n",
    "df_pbp.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ pbp_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Power Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"pbp_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"pbp_final.csv\")\n",
    "\n",
    "df_pbp = pd.read_csv(input_path)\n",
    "\n",
    "df_pbp['date_source'] = pd.to_datetime(df_pbp['date_source'], format='%Y%m%d')\n",
    "df_pbp['match_id'] = df_pbp['match_id'].astype(int)\n",
    "df_pbp['home_point'] = df_pbp['home_point'].astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].astype(int)\n",
    "df_pbp['set_id'] = df_pbp['set_id'].astype(int)\n",
    "df_pbp['game_id'] = df_pbp['game_id'].astype(int)\n",
    "df_pbp['point_id'] = df_pbp['point_id'].astype(int)\n",
    "\n",
    "df_pbp.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ pbp_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Question 1 ‚Äî How many tennis players are included in the dataset?\n",
    "\n",
    "To find the number of unique tennis players, we combine the home and away team tables, remove duplicate players based on `player_id`, and count how many unique players remain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "# Load cleaned & normalized player data\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# Combine home and away players\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# Keep only unique players\n",
    "unique_players = players.drop_duplicates(subset=\"player_id\")\n",
    "\n",
    "# Count unique players\n",
    "total_unique_players = unique_players.shape[0]\n",
    "\n",
    "print(\"total unique players =\",total_unique_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "###  Question 2 ‚Äî What is the average height of the players?\n",
    "The goal of this question is to calculate the average height of the tennis players in the dataset.\n",
    "To do this, we first need to create a unique players table so that players who are duplicates in home and away are not counted again.\n",
    "Then we correct invalid heights (such as 0 or NaN) and calculate the true average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "# load data\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# combine & remove duplicate players\n",
    "players = pd.concat([df_home , df_away], ignore_index=True)\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\").copy()\n",
    "\n",
    "# replace zeros with NaN\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].replace(0, np.nan)\n",
    "\n",
    "# fill missing heights with mean\n",
    "mean_height = players_unique[\"height\"].mean(skipna=True)\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].fillna(mean_height)\n",
    "\n",
    "# Calculate average height\n",
    "average_height = players_unique[\"height\"].mean()\n",
    "print(\"Average height =\" , average_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The histogram + KDE curve helps us observe:  \n",
    "- The central height tendency (mean around ~182 cm)  \n",
    "- Spread of heights  \n",
    "- Possible outliers  \n",
    "- Whether the distribution is normal or skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Player Height Distribution\")\n",
    "sns.histplot(players_unique[\"height\"], kde=True, bins=30)\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Insight\n",
    "The average tennis player height is around **182 cm**.  \n",
    "The distribution appears slightly right-skewed, meaning a small number of players are significantly taller than the average.\n",
    "\n",
    "Most players fall between **175‚Äì190 cm**, which aligns with typical professional tennis standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3523b",
   "metadata": {},
   "source": [
    "# Questions Q3 to Q9\n",
    "\n",
    "The notebook assumes the following project structure:\n",
    "- This file lives in the `notebooks/` directory\n",
    "- Cleaned CSV files live in `../data/clean/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404dd3c",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67170740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "BASE_DIR = \"your clean folder path\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'clean')\n",
    "\n",
    "stats = pd.read_csv(os.path.join(DATA_DIR, 'statistics_final.csv'))\n",
    "home = pd.read_csv(os.path.join(DATA_DIR, 'home_team_final.csv'))\n",
    "away = pd.read_csv(os.path.join(DATA_DIR, 'away_team_final.csv'))\n",
    "event = pd.read_csv(os.path.join(DATA_DIR, 'event_final.csv'))\n",
    "tourn = pd.read_csv(os.path.join(DATA_DIR, 'tournament_final.csv'))\n",
    "time_df = pd.read_csv(os.path.join(DATA_DIR, 'time_final.csv'))\n",
    "\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6ff03",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_winners(event, home, away):\n",
    "    \"\"\"Reconstruct winner player (id + name) for each match.\n",
    "\n",
    "    Uses `winner_code` from the event table:\n",
    "    - 1.0 ‚Üí home player wins\n",
    "    - 2.0 ‚Üí away player wins\n",
    "    \"\"\"\n",
    "    winners = event[event['winner_code'].isin([1.0, 2.0])][['match_id', 'winner_code']].copy()\n",
    "\n",
    "    winners = winners.merge(\n",
    "        home[['match_id', 'player_id', 'full_name']],\n",
    "        on='match_id',\n",
    "        how='left',\n",
    "        suffixes=('', '_home'),\n",
    "    )\n",
    "\n",
    "    winners = winners.merge(\n",
    "        away[['match_id', 'player_id', 'full_name']],\n",
    "        on='match_id',\n",
    "        how='left',\n",
    "        suffixes=('_home', '_away'),\n",
    "    )\n",
    "\n",
    "    def pick_winner(row):\n",
    "        if row['winner_code'] == 1.0:\n",
    "            return pd.Series({'winner_id': row['player_id_home'], 'winner_name': row['full_name_home']})\n",
    "        elif row['winner_code'] == 2.0:\n",
    "            return pd.Series({'winner_id': row['player_id_away'], 'winner_name': row['full_name_away']})\n",
    "        return pd.Series({'winner_id': np.nan, 'winner_name': np.nan})\n",
    "\n",
    "    winners_ids = winners.join(winners.apply(pick_winner, axis=1))\n",
    "    winners_ids = winners_ids.dropna(subset=['winner_id', 'winner_name'])\n",
    "    return winners_ids[['match_id', 'winner_id', 'winner_name', 'winner_code']]\n",
    "\n",
    "\n",
    "def add_duration(time_df):\n",
    "    t = time_df.copy()\n",
    "    t['duration_sec'] = t[['period_1', 'period_2', 'period_3']].fillna(0).sum(axis=1)\n",
    "    return t\n",
    "\n",
    "\n",
    "winners_ids = build_winners(event, home, away)\n",
    "winners_ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b83201",
   "metadata": {},
   "source": [
    "## Question 3 ‚Äî Which player has the highest number of wins?\n",
    "\n",
    "To answer this question, we:\n",
    "1. Reconstruct the winner of each match from the `event` table using `winner_code`.\n",
    "2. Join with the home and away player tables to get the winner's name and ID.\n",
    "3. Group by player and count the number of wins.\n",
    "4. Sort the results and inspect the top 10 players.\n",
    "\n",
    "This shows who is most successful in terms of raw match wins in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_players = (\n",
    "    winners_ids.groupby(['winner_id', 'winner_name']).size().reset_index(name='wins')\n",
    "    .sort_values('wins', ascending=False)\n",
    ")\n",
    "\n",
    "# Drop unknown placeholder names\n",
    "top_players = top_players[top_players['winner_name'] != 'Unknown']\n",
    "\n",
    "top_player = top_players.iloc[0]\n",
    "top_10 = top_players.head(10)\n",
    "\n",
    "print('Q3  Top Player by Wins')\n",
    "print('--------------------------------')\n",
    "print(f\"Top player: {top_player['winner_name']}\")\n",
    "print(f\"Total wins: {int(top_player['wins'])}\")\n",
    "print('\\nTop 10 players by wins:')\n",
    "display(top_10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_10['winner_name'], top_10['wins'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Number of wins')\n",
    "plt.title('Top 10 Players by Match Wins')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdc4bb",
   "metadata": {},
   "source": [
    "### Insight\n",
    "Based on the output above:\n",
    "- **Dmitry Popko** is the most successful player in this sample, with **28 match wins**.\n",
    "- Several other players cluster closely behind with around **18‚Äì22 wins**, indicating a fairly competitive field at the top.\n",
    "- The distribution of wins is right‚Äëskewed: most players win relatively few matches, while only a handful accumulate a large number of victories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931baf2d",
   "metadata": {},
   "source": [
    "## Question 4 ‚Äî What is the longest match in the dataset?\n",
    "\n",
    "Here we want to find the match with the maximum total duration.\n",
    "To do this we:\n",
    "1. Compute a `duration_sec` column in the time table by summing `period_1`, `period_2`, and `period_3`.\n",
    "2. Identify the row with the maximum duration.\n",
    "3. Join back to the event and tournament tables to retrieve metadata such as start datetime and tournament id.\n",
    "4. Plot the overall distribution of match durations and mark the longest match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_with_dur = add_duration(time_df)\n",
    "idx_longest = time_with_dur['duration_sec'].idxmax()\n",
    "longest_row = time_with_dur.loc[idx_longest]\n",
    "\n",
    "match_id_long = int(longest_row['match_id'])\n",
    "duration_long = int(longest_row['duration_sec'])\n",
    "\n",
    "ev_long = event[event['match_id'] == match_id_long].iloc[0]\n",
    "tr_long = tourn[tourn['match_id'] == match_id_long].iloc[0]\n",
    "\n",
    "hours = duration_long // 3600\n",
    "minutes = (duration_long % 3600) // 60\n",
    "\n",
    "print('Q4  Longest Match')\n",
    "print('-------------------')\n",
    "print(f\"Match ID: {match_id_long}\")\n",
    "print(f\"Duration: {duration_long} seconds (~{hours} hours {minutes} minutes)\")\n",
    "print(f\"Tournament ID: {int(tr_long['tournament_id'])}\")\n",
    "print(f\"Start datetime: {ev_long['start_datetime']}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "durations = time_with_dur['duration_sec'].dropna()\n",
    "plt.hist(durations, bins=30)\n",
    "plt.axvline(duration_long, linestyle='--')\n",
    "plt.xlabel('Match duration (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Match Durations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66070c56",
   "metadata": {},
   "source": [
    "### Insight\n",
    "The longest match in the dataset is **match 12185562**, played in tournament **127957**, with a recorded duration of about **177,131 seconds (~49 hours)**. This value is clearly unrealistic for real tennis, which strongly suggests that either:\n",
    "- the time periods were not recorded in true seconds, or\n",
    "- there is a data quality issue for this particular match.\n",
    "\n",
    "The histogram shows that most matches are concentrated at much shorter durations, reinforcing the idea that the longest‚Äëmatch value is an outlier caused by data issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4420091",
   "metadata": {},
   "source": [
    "## Question 5 ‚Äî What is the distribution of the number of sets played per match?\n",
    "\n",
    "In this question we want to understand how many sets matches usually last.\n",
    "Steps:\n",
    "1. For each match, count how many of `period_1`, `period_2`, and `period_3` are non‚Äënull.\n",
    "2. Treat that count as the number of sets played.\n",
    "3. Build a frequency table of set counts (0, 1, 2, 3).\n",
    "4. Report which number of sets is most common and visualise the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sets(row):\n",
    "    return (~row[['period_1', 'period_2', 'period_3']].isna()).sum()\n",
    "\n",
    "time_sets = time_df.copy()\n",
    "time_sets['set_count'] = time_sets.apply(count_sets, axis=1)\n",
    "\n",
    "set_counts = time_sets['set_count'].value_counts().sort_index()\n",
    "valid = set_counts[set_counts.index > 0]\n",
    "typical_sets = int(valid.idxmax()) if not valid.empty else None\n",
    "\n",
    "print('Q5  Set Count Distribution')\n",
    "print('----------------------------')\n",
    "print('Most common number of sets in a match:', typical_sets)\n",
    "print('\\nFull distribution:')\n",
    "display(set_counts)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(set_counts.index.astype(str), set_counts.values)\n",
    "plt.xlabel('Number of sets')\n",
    "plt.ylabel('Number of matches')\n",
    "plt.title('Set Count Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c113e",
   "metadata": {},
   "source": [
    "### Insight\n",
    "The distribution of set counts is roughly as follows:\n",
    "- **2‚Äëset matches:** 7,110 occurrences\n",
    "- **3‚Äëset matches:** 3,133 occurrences\n",
    "- **1‚Äëset matches:** 84 occurrences\n",
    "- **0 sets:** 6,546 rows (likely incomplete or invalid records)\n",
    "\n",
    "Ignoring the invalid 0‚Äëset entries, the most typical match format in this dataset is a **2‚Äëset match**, which is consistent with best‚Äëof‚Äë3 matches where many are decided in straight sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb88fd5",
   "metadata": {},
   "source": [
    "## Question 6 ‚Äî Which country has the most wins?\n",
    "\n",
    "Here we aggregate wins at the country level.\n",
    "Method:\n",
    "1. Take the winners table and match winner names back to the home and away team tables.\n",
    "2. Use player country from whichever side (home/away) matches the winner.\n",
    "3. Count wins per country.\n",
    "4. Sort to find the top countries and plot the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = winners_ids[['match_id', 'winner_name']].copy()\n",
    "home_subset = home[['match_id', 'full_name', 'country']].copy()\n",
    "away_subset = away[['match_id', 'full_name', 'country']].copy()\n",
    "\n",
    "w_home = w.merge(home_subset, left_on=['match_id', 'winner_name'], right_on=['match_id', 'full_name'], how='left')\n",
    "w_merged = w_home.merge(\n",
    "    away_subset,\n",
    "    left_on=['match_id', 'winner_name'],\n",
    "    right_on=['match_id', 'full_name'],\n",
    "    how='left',\n",
    "    suffixes=('_home', '_away'),\n",
    ")\n",
    "\n",
    "w_merged['country'] = w_merged['country_home'].combine_first(w_merged['country_away'])\n",
    "w_clean = w_merged[w_merged['country'].notna() & (w_merged['country'] != 'Unknown')]\n",
    "\n",
    "country_wins = (\n",
    "    w_clean.groupby('country')\n",
    "    .size()\n",
    "    .reset_index(name='wins')\n",
    "    .sort_values('wins', ascending=False)\n",
    ")\n",
    "\n",
    "print('Q6   Best Country by Wins')\n",
    "print('---------------------------')\n",
    "display(country_wins.head(10))\n",
    "\n",
    "top_countries = country_wins.head(10)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_countries['country'], top_countries['wins'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of wins')\n",
    "plt.title('Top Countries by Match Wins')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2e64d",
   "metadata": {},
   "source": [
    "### Insight\n",
    "From the aggregated results:\n",
    "- **France** is the leading country with **973 recorded wins**.\n",
    "- It is followed closely by **Italy (918 wins)** and the **USA (901 wins)**.\n",
    "- Other strong tennis nations such as Russia, Argentina, Germany, Japan and Spain also appear in the top 10.\n",
    "\n",
    "This ranking reflects both the strength and the activity level of players from these countries in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6813988",
   "metadata": {},
   "source": [
    "## Question 7 ‚Äî What is the average number of aces per match?\n",
    "\n",
    "Aces are a key indicator of serving power and effectiveness.\n",
    "Approach:\n",
    "1. Filter the statistics table to keep only rows where `statistic_name == 'aces'`.\n",
    "2. For each match, compute `total_aces = home_value + away_value`.\n",
    "3. Take the mean of `total_aces` across all matches.\n",
    "4. Plot the distribution of aces per match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aces = stats[stats['statistic_name'] == 'aces'].copy()\n",
    "aces['total_aces'] = aces['home_value'] + aces['away_value']\n",
    "\n",
    "avg_aces = aces['total_aces'].mean()\n",
    "print('Q7   Average Aces per Match')\n",
    "print('-----------------------------')\n",
    "print('Average number of aces per match:', avg_aces)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(aces['total_aces'].dropna(), bins=30)\n",
    "plt.xlabel('Total aces per match')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Aces per Match')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44de95",
   "metadata": {},
   "source": [
    "### Insight\n",
    "The average match in this dataset features approximately **3.8 aces** in total.\n",
    "The histogram shows a right‚Äëskewed distribution:\n",
    "- Many matches have only **1‚Äì3 aces**.\n",
    "- A smaller number of matches have very high ace counts, up to **52 aces**, which pull the mean upwards.\n",
    "This pattern is typical of serve‚Äëdominated surfaces or players with particularly strong serves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e864a",
   "metadata": {},
   "source": [
    "## Question 8 ‚Äî How do double faults differ between male and female players?\n",
    "\n",
    "Double faults are costly errors on serve. We want to compare average double faults by gender.\n",
    "Steps:\n",
    "1. Filter the statistics table to `double_faults`.\n",
    "2. Reshape home and away values into a long format (one row per player side).\n",
    "3. Attach player gender from the home and away team tables.\n",
    "4. Compute the mean number of double faults separately for male and female players.\n",
    "5. Visualise the comparison with a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaults = stats[stats['statistic_name'] == 'double_faults'][['match_id', 'home_value', 'away_value']].copy()\n",
    "\n",
    "dfaults_melt = dfaults.melt(\n",
    "    id_vars='match_id',\n",
    "    value_vars=['home_value', 'away_value'],\n",
    "    var_name='side',\n",
    "    value_name='double_faults',\n",
    ")\n",
    "\n",
    "home_gender = home[['match_id', 'gender']].copy()\n",
    "home_gender['side'] = 'home_value'\n",
    "away_gender = away[['match_id', 'gender']].copy()\n",
    "away_gender['side'] = 'away_value'\n",
    "\n",
    "gender_map = pd.concat([home_gender, away_gender], ignore_index=True)\n",
    "dfaults_melt = dfaults_melt.merge(gender_map, on=['match_id', 'side'], how='left')\n",
    "\n",
    "df_clean = dfaults_melt[dfaults_melt['gender'].isin(['M', 'F'])]\n",
    "mean_df = df_clean.groupby('gender')['double_faults'].mean()\n",
    "\n",
    "print('Q8  Double Faults by Gender')\n",
    "print('-----------------------------')\n",
    "for g, v in mean_df.items():\n",
    "    print(f\"Average double faults for {g}: {v}\")\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(mean_df.index, mean_df.values)\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Average double faults')\n",
    "plt.title('Average Double Faults by Gender')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b38f4",
   "metadata": {},
   "source": [
    "### Insight\n",
    "Using the current dataset:\n",
    "- **Female players (F)** commit on average about **2.19 double faults per match**.\n",
    "- **Male players (M)** commit on average about **1.70 double faults per match**.\n",
    "\n",
    "So women in this sample average slightly more double faults than men. However, the difference is not huge, and interpretation should consider factors such as match format, surface, and tournament level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624272d",
   "metadata": {},
   "source": [
    "## Question 9 ‚Äî Which tournament has the most wins in its best month?\n",
    "\n",
    "For each tournament, we would like to know in which month it saw the most wins (i.e., matches played), and then identify the tournament whose best month is the most active overall.\n",
    "Method:\n",
    "1. Merge the winners table with the tournament table to get `tournament_id` per match.\n",
    "2. Attach the match start datetime and extract `month`.\n",
    "3. Count wins per `(tournament_id, month)` pair.\n",
    "4. For each tournament, keep only the month with the maximum number of wins.\n",
    "5. Sort tournaments by this maximum and inspect the top ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = winners_ids[['match_id', 'winner_id']].copy()\n",
    "wm = wm.merge(tourn[['match_id', 'tournament_id']], on='match_id', how='left')\n",
    "wm = wm.merge(event[['match_id', 'start_datetime']], on='match_id', how='left')\n",
    "\n",
    "wm['start_datetime'] = pd.to_datetime(wm['start_datetime'])\n",
    "wm['month'] = wm['start_datetime'].dt.to_period('M')\n",
    "\n",
    "tm_counts = wm.groupby(['tournament_id', 'month']).size().reset_index(name='wins')\n",
    "\n",
    "best_months = tm_counts.sort_values(['tournament_id', 'wins'], ascending=[True, False]) \\\n",
    "                        .drop_duplicates(subset=['tournament_id'])\n",
    "\n",
    "best_months_sorted = best_months.sort_values('wins', ascending=False)\n",
    "top_t = best_months_sorted.iloc[0]\n",
    "\n",
    "print('Q9  Tournament with Most Wins in Its Best Month')\n",
    "print('-------------------------------------------------')\n",
    "print('Tournament ID:', int(top_t['tournament_id']))\n",
    "print('Best month:', str(top_t['month']))\n",
    "print('Number of wins in that month:', int(top_t['wins']))\n",
    "\n",
    "top_n = best_months_sorted.head(10).copy()\n",
    "top_n['label'] = top_n['tournament_id'].astype(str) + ' | ' + top_n['month'].astype(str)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_n['label'], top_n['wins'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Tournament | Month')\n",
    "plt.ylabel('Wins in best month')\n",
    "plt.title('Top Tournaments by Wins in Their Best Month')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f839d63",
   "metadata": {},
   "source": [
    "### Insight\n",
    "Several tournaments achieve very high activity in their peak month. The leading tournaments in this dataset (for example, IDs **127289**, **127288**, **127762**, and **127782**) each record around **95 wins in March 2024**.\n",
    "\n",
    "This suggests that March 2024 is a particularly dense period in the calendar, with multiple tournaments running simultaneously and generating a large number of completed matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Question 10 ‚Äî Correlation Between Player Height and Ranking\n",
    "Interpretation\n",
    "\n",
    "We want to check whether taller players tend to have higher or lower rankings.\n",
    "We combine home & away players, remove duplicates, clean invalid values, and compute Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "# Load datasets\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# Merge home + away\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# Keep unique players\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\")\n",
    "\n",
    "# Replace invalid zeros with NaN\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].replace(0, np.nan)\n",
    "players_unique.loc[:, \"current_rank\"] = players_unique[\"current_rank\"].replace(0, np.nan)\n",
    "\n",
    "# Drop rows with missing required values\n",
    "clean_players = players_unique.dropna(subset=[\"height\", \"current_rank\"])\n",
    "\n",
    "# Compute correlation\n",
    "correlation = clean_players[\"height\"].corr(clean_players[\"current_rank\"], method=\"pearson\")\n",
    "\n",
    "print(\"correlation =\" , correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Correlation = 0.10355\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "There is no meaningful correlation between player height and ranking.\n",
    "Height does not significantly influence global ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(\n",
    "    x=\"height\",\n",
    "    y=\"current_rank\",\n",
    "    data=clean_players,\n",
    "    scatter_kws={\"alpha\":0.4},\n",
    "    line_kws={\"color\":\"red\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Height vs Ranking\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Ranking (Lower is Better)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "###  Insight\n",
    "The correlation coefficient was approximately **0.10**, indicating a **very weak positive correlation**.\n",
    "\n",
    "This means **taller players tend to have slightly worse rankings**, but the effect is extremely small.  \n",
    "\n",
    "Height does NOT strongly predict performance or ranking in professional tennis.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Question 11 ‚Äî What is the average duration of matches?\n",
    "\n",
    "To calculate the average match duration, we used the `time_final.csv` dataset, which contains\n",
    "the duration of each period inside a tennis match:\n",
    "\n",
    "- `period_1`\n",
    "- `period_2`\n",
    "- `period_3`\n",
    "\n",
    "These periods contain the **duration in seconds**.\n",
    "\n",
    "#### **Steps**\n",
    "1. Replace NaN values in period columns with 0.  \n",
    "2. Compute total match duration:  `duration_seconds = period_1 + period_2 + period_3`  \n",
    "3. Keep only matches where duration > 0.  \n",
    "4. Compute the mean duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "df_time = pd.read_csv(os.path.join(base, \"time_final.csv\"))\n",
    "\n",
    "# Replace NaN periods with 0\n",
    "for col in [\"period_1\", \"period_2\", \"period_3\"]:\n",
    "    df_time[col] = df_time[col].fillna(0)\n",
    "\n",
    "# Compute duration in seconds\n",
    "df_time.loc[:, \"duration_seconds\"] = df_time[\"period_1\"] + df_time[\"period_2\"] + df_time[\"period_3\"]\n",
    "\n",
    "# Filter out zeros (invalid matches)\n",
    "df_valid = df_time[df_time[\"duration_seconds\"] > 0].copy()\n",
    "df_valid[\"duration_minutes\"] = df_valid[\"duration_seconds\"] / 60\n",
    "\n",
    "# Average duration\n",
    "avg_sec = df_valid[\"duration_seconds\"].mean()\n",
    "avg_minutes = avg_sec / 60\n",
    "avg_hours = avg_minutes / 60\n",
    "\n",
    "print(\"Average duration (seconds)=\", avg_sec)\n",
    "print(\"Average duration (minutes)=\", avg_minutes)\n",
    "print(\"Average duration (hours)=\", avg_hours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- **Average duration (seconds):** 6705.87  \n",
    "- **Average duration (minutes):** 111.76  \n",
    "- **Average duration (hours):** 1.86  \n",
    "\n",
    "#### **Average tennis match duration ‚âà 1 hour and 52 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Distribution of Match Duration\")\n",
    "sns.histplot(df_valid[\"duration_minutes\"], kde=True, bins=40)\n",
    "plt.xlabel(\"Duration (minutes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "###  Insight\n",
    "\n",
    "The distribution shows:  \n",
    "- Most matches are between **60 and 130 minutes**  \n",
    "- A small number of matches last more than **180 minutes**  \n",
    "- Very short or extremely long matches are rare  \n",
    "  \n",
    "  --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Question 12 - What is the average number of games per set in men's matches compared to women's matches?\n",
    "\n",
    "To calculate the average number of games per set for men's and women's matches, we used three main datasets: `home_team_final.csv`, `away_team_final.csv`, and `pbp_final.csv`.\n",
    "\n",
    "The `gender` column in the `home_team_final.csv` and `away_team_final.csv` datasets contains information about the **gender** of the players, and in the `pbp_final.csv` dataset we can also have `information about each game` to count it into women's and men's matches.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading datasets and storing them in code as dataframes\n",
    "2. Create a `gender` dataframe that contains information about the gender and matches played by home and away players.\n",
    "3. Obtaining the gender of each match using player grouping and getting the mode of the players for each match with the code `df_gender = df_gender.groupby(\"match_id\")['gender'].apply(lambda x: x.mode()[0]).reset_index()`\n",
    "4. Counting games per match and storing them in `df_games`\n",
    "5. Merging the `df_gender` and `df_games` dataframes for the final calculation\n",
    "6. Obtain the final result by grouping `df_merged` by gender and averaging for each gender (ignoring data where the `gender` column is unknown)\n",
    "7. Obtaining the ratio of gender-unknown data to total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\"))\n",
    "df_pbp = pd.read_csv(os.path.join(etl_files_path, \"pbp_final.csv\"))\n",
    "\n",
    "df_gender = pd.concat(\n",
    "    [df_home[['match_id', 'gender']],\n",
    "     df_away[['match_id', 'gender']]],\n",
    "    ignore_index=True)\n",
    "\n",
    "df_gender = df_gender.groupby(\"match_id\")['gender'].apply(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "df_games = df_pbp.groupby(['match_id', 'set_id',])['game_id'].nunique().reset_index(name='games_in_set')\n",
    "\n",
    "df_merged = df_games.merge(df_gender, on='match_id', how='inner')\n",
    "\n",
    "result = df_merged.groupby('gender')['games_in_set'].mean().drop('Unknown')\n",
    "\n",
    "stat = df_merged.groupby('gender')['games_in_set']\n",
    "\n",
    "ratio_of_unknown = (df_merged['gender'] == 'Unknown').sum() / len(df_merged) * 100\n",
    "\n",
    "print(result)\n",
    "print(ratio_of_unknown)\n",
    "print('minimum games in a match', stat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- The average number of games per set in men's matches is approximately equal to **9.27**\n",
    "- The average number of games per set in women's matches is approximately equal to **8.90**\n",
    "- The highest number of games in a match for both women and men was **13**.\n",
    "\n",
    "#### The proportion of players whose gender was unknown to the total dataset is approximately equal to **5.1%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.barplot(\n",
    "    y=['Male', 'Female'],\n",
    "    x=result.values,\n",
    "    ax=ax,\n",
    "    color='skyblue',\n",
    "    hue=result.index,\n",
    "    edgecolor = 'gray',\n",
    "    palette=['skyblue', 'lightpink'],\n",
    "    alpha = 0.9\n",
    ")\n",
    "ax.set_title('Average Number of Games per Match by Gender', fontsize=16, loc='left')\n",
    "ax.set_ylabel('Gender', fontsize=14, loc='bottom', color='gray')\n",
    "ax.set_xlabel('Games per Match', fontsize=14, loc=\"left\", color='gray')\n",
    "ax.tick_params(axis='both', which='major', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "ax.set_xticks(np.arange(0, 10, 1))\n",
    "ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- The average number of games in a match between women and men is not much different **(around 0.3 games per match)**.\n",
    "- About **5.1%** of players had their gender unknown in the dataset and were not included in this statistic.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Question 13 ‚Äî What is the distribution of left-handed versus right-handed players?\n",
    "\n",
    "To calculate the distribution of left-handed versus right-handed players, we use two datasets, `home_team_final.csv` and `away_team_final.csv`, which contain player information.\n",
    "\n",
    "In both of these datasets, there is a column called `plays` that stores information about whether the player is left-handed or right-handed.\n",
    "\n",
    "#### **Steps**\n",
    "1. Read the dataset and store it in code as data frames, uniquely considering each player and extracting only the plays column.\n",
    "2. Creating df_all, which is a dataframe containing information about all players.\n",
    "3. Create a distribution dataframe that counts and stores data for left-handed or right-handed players or for players whose status is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\")).drop_duplicates(subset='player_id')['plays']\n",
    "df_away = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\")).drop_duplicates(subset='player_id')['plays']\n",
    "\n",
    "df_all = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "df_clean = df_all[df_all != 'unknown']\n",
    "\n",
    "distribution = df_all.value_counts(dropna=True)\n",
    "print(distribution)\n",
    "\n",
    "ratio_of_unknown = (df_all == 'unknown').sum() / len(df_all) * 100\n",
    "print('Ratio of unknown plays:', ratio_of_unknown)\n",
    "\n",
    "distribution = distribution.drop('unknown', errors='ignore')\n",
    "\n",
    "ratio_of_right_handed_without_unknown = (df_clean == 'right-handed').sum() / len(df_clean) * 100\n",
    "print('Ratio of right-handed players:', ratio_of_right_handed_without_unknown)\n",
    "\n",
    "ratio_of_left_handed_without_unknown = (df_clean == 'left-handed').sum() / len(df_clean) * 100\n",
    "print('Ratio of left-handed players:', ratio_of_left_handed_without_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- Without taking into account uncertain data, approximately **88.54% of players are right-handed**\n",
    "-  and **11.45% of them are left-handed**.\n",
    "\n",
    "#### Unfortunately, about **55.94%** of all players have data regarding their right- or left-handedness unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    y=distribution.index,\n",
    "    x=distribution.values,\n",
    "    palette=['skyblue', 'lightgreen'],\n",
    "    hue=distribution.index,\n",
    "    alpha=0.9,\n",
    "    edgecolor = 'gray'\n",
    "    )\n",
    "ax.set_title('Distribution of left-handed versus right-handed players', fontsize=16, loc='left')\n",
    "ax.set_ylabel('Handedness', fontsize=14, loc='bottom', color='gray')\n",
    "ax.set_xlabel('Number of Players', fontsize=14, loc=\"left\", color='gray')\n",
    "ax.tick_params(axis='both', which='major', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- Of all players, the left-handed or right-handed status of **2,749** is unknown, with **1,917** being right-handed and **248** being left-handed.\n",
    "- The total number of players considered in this section was **4914**\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Question 14 ‚Äî What is the most common type of surface used in tournaments?\n",
    "\n",
    "To calculate the most common field type in tournaments, we only need the `tournament_final.csv` dataset.\n",
    "\n",
    "In this dataset, there is a column called `ground_type` through which you can find out the type of ground on which the matches were held.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading a dataset and storing it in a dataframe\n",
    "2. Counting `ground_type`s in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tournaments = pd.read_csv(os.path.join(etl_files_path, \"tournament_final.csv\"))\n",
    "\n",
    "tournament_groundtype_counts = df_tournaments['ground_type'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "print(f'The most common ground type is: {tournament_groundtype_counts.idxmax()} with {tournament_groundtype_counts.max()} tournaments.')\n",
    "\n",
    "ratio_of_unknown = (df_tournaments['ground_type'] == 'Unknown').sum() / len(df_tournaments) * 100\n",
    "print('Ratio of unknown ground types:', ratio_of_unknown)\n",
    "print(tournament_groundtype_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- The most common ground type is: Hardcourt outdoor\n",
    "- Hardcourt outdoor has been used as a playing surface in 8116 tournaments\n",
    "\n",
    "#### Only 1.61% of the data had an unknown playing surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    y=tournament_groundtype_counts.index,\n",
    "    x=tournament_groundtype_counts.values,\n",
    "    palette=['skyblue', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray'],\n",
    "    hue=tournament_groundtype_counts.index,\n",
    "    edgecolor='gray',\n",
    "    alpha = 0.9\n",
    "    )\n",
    "ax.set_title('Most common playing surfaces', fontsize=16, loc='left')\n",
    "ax.set_ylabel('Playing Surface', fontsize=14, loc='bottom', color='gray')\n",
    "ax.set_xlabel('Number of Tournaments', fontsize=14, loc=\"left\", color='gray')\n",
    "ax.tick_params(axis='both', which='major', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- There are about **16,873** tournaments in the dataset, with the most common playing surface among them being **Hardcourt outdoor**, which was present as the playing surface in **8,116** tournaments.\n",
    "- The least popular playing surface among tournaments is **Green clay**, which has only been used as a playing surface in **18** tournaments.\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### Question 15 ‚Äî How many distinct countries are represented in the dataset?\n",
    "\n",
    "To calculate the individual countries in the dataset, we need to refer back to the `home_team_final.csv` and `away_team_final`.csv datasets.\n",
    "\n",
    "In both of these datasets, there is a column called `country` for each player, which specifies the player's nationality and shows how many distinct countries there are in our data.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading datasets and storing only their `country` column in dataframes\n",
    "2. Creating `df_country` which is a merged version of home and away countries\n",
    "3. Counting distinct countries in `df_country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\"))[\"country\"]\n",
    "df_away = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\"))[\"country\"]\n",
    "\n",
    "df_country = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "country_counts = df_country.value_counts().sort_values(ascending=False)\n",
    "\n",
    "country_counts_for_display = country_counts.head(10).drop(labels=[\"Unknown\"], errors='ignore')\n",
    "\n",
    "ratio_of_unknown = (df_country == 'Unknown').sum() / len(df_country) * 100\n",
    "print('Ratio of unknown countries:', ratio_of_unknown)\n",
    "print(country_counts.info())\n",
    "print(country_counts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- There are **101 distinct countries** in our dataset, excluding unknown data.\n",
    "- **France** has the highest number of players in our dataset.\n",
    "\n",
    "#### About 29.72% of the data related to countries is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.barplot(ax=ax,\n",
    "            x=country_counts_for_display.index,\n",
    "            y=country_counts_for_display.values,\n",
    "            hue=country_counts_for_display.index,\n",
    "            edgecolor='gray',\n",
    "            palette=['skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue'],\n",
    "            alpha=0.9)\n",
    "ax.set_title(f'Top 10 Countries of Teams\\nRatio of \"Unknown\" countries: {ratio_of_unknown:.2f}%\\nThere are 101 distinct countries in the dataset.', loc='left', fontsize=16, color='gray', pad=20)\n",
    "ax.set_xlabel('Country', loc='left')\n",
    "ax.set_ylabel('Number of Teams', loc='bottom')\n",
    "ax.tick_params(axis='both', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- Of the 101 distinct countries, **France** had the highest number of players in the dataset.\n",
    "- **Qatar**, **Bahamas** and **Haiti**, which are at the bottom of the table, each have only **1** player in the dataset.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "### Question 16 ‚Äî Which player has the highest winning percentage against top 10 ranked opponents?\n",
    "\n",
    "To find the player with the highest win rate against the 10 highest ranked players in the dataset, we need to use the datasets `home_team_final.csv`, `away_team_final.csv`, and `event_final.csv`.\n",
    "\n",
    "To access player information, we use the home and away datasets, and to access match information, especially match winner information, we use the `winner_code` column event dataset.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading datasets and saving them in dataframes\n",
    "2. Creating a `players` Table by Merging Home and Away\n",
    "3. Extracting the top 10 players based on rank using the `current_rank` column\n",
    "4. Extract all matches played by the top 10 players\n",
    "5. Extracting top 10 competitors in matches alongside themselves\n",
    "6. Merge the result with the event table to have the winner_code and the desired players in one place.\n",
    "7. Determine if each player won their match\n",
    "8. Filter NON‚ÄìTop10 players only (the opponents)\n",
    "9. Calculate win percentage vs Top10\n",
    "10. Find the best player (sort by win rate and number pf matchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home  = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\"))\n",
    "df_away  = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\"))\n",
    "df_event = pd.read_csv(os.path.join(etl_files_path, \"event_final.csv\"))\n",
    "\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# keep valid players only (remove rows with Unknown or rank=0)\n",
    "players = players[(players[\"player_id\"] != \"Unknown\") & (players[\"current_rank\"] > 0)].copy()\n",
    "\n",
    "# Create 'side' column: home=1, away=2\n",
    "home_ids = df_home[\"player_id\"].tolist()\n",
    "players[\"side\"] = players[\"player_id\"].apply(lambda x: 1 if x in home_ids else 2)\n",
    "\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\")\n",
    "top10 = players_unique.sort_values(\"current_rank\", ascending=False).head(10)\n",
    "\n",
    "top10_ids = top10[\"player_id\"].tolist()\n",
    "\n",
    "matches_with_top10 = players[players[\"player_id\"].isin(top10_ids)][\"match_id\"].unique()\n",
    "\n",
    "subset_matches = players[players[\"match_id\"].isin(matches_with_top10)].copy()\n",
    "\n",
    "subset_matches = subset_matches.merge(\n",
    "    df_event[[\"match_id\", \"winner_code\"]],\n",
    "    on=\"match_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "subset_matches[\"is_winner\"] = (\n",
    "    ((subset_matches[\"side\"] == 1) & (subset_matches[\"winner_code\"] == 1)) |\n",
    "    ((subset_matches[\"side\"] == 2) & (subset_matches[\"winner_code\"] == 2))\n",
    ")\n",
    "\n",
    "opponents = subset_matches[~subset_matches[\"player_id\"].isin(top10_ids)].copy()\n",
    "\n",
    "win_rate = opponents.groupby(\"player_id\").agg(\n",
    "    matches_vs_top10=(\"match_id\", \"count\"),\n",
    "    wins_vs_top10=(\"is_winner\", \"sum\")\n",
    ")\n",
    "\n",
    "win_rate[\"win_percentage_vs_top10\"] = (\n",
    "    win_rate[\"wins_vs_top10\"] / win_rate[\"matches_vs_top10\"]\n",
    ") * 100\n",
    "\n",
    "# Add minimum matches threshold (recommended >= 5)\n",
    "threshold = 5\n",
    "\n",
    "win_rate_filtered = win_rate[win_rate[\"matches_vs_top10\"] >= threshold]\n",
    "\n",
    "# If no player passes threshold, fall back to >=2\n",
    "if len(win_rate_filtered) == 0:\n",
    "    win_rate_filtered = win_rate[win_rate[\"matches_vs_top10\"] >= 2]\n",
    "\n",
    "# Filter players that played minimum 2 matches vs top10\n",
    "win_rate_filtered = win_rate[win_rate[\"matches_vs_top10\"] >= 2]\n",
    "\n",
    "# Sort by win rate first, then number of matches\n",
    "best_player = win_rate_filtered.sort_values(\n",
    "    [\"win_percentage_vs_top10\", \"matches_vs_top10\"],\n",
    "    ascending=[False, False]\n",
    ").head(1)\n",
    "\n",
    "players_unique = players_unique[[\"player_id\", \"full_name\", \"current_rank\"]]\n",
    "\n",
    "best_player_named = best_player.reset_index().merge(\n",
    "    players_unique,\n",
    "    on=\"player_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Best player against top 10: {best_player_named['full_name'].values[0]} with ID: {best_player_named['player_id'].values[0]}\")\n",
    "print(\"he have played\", best_player_named['matches_vs_top10'].values[0], \"matches against top 10 players\")\n",
    "print(f\"And he won {best_player_named['wins_vs_top10'].values[0]} of them.\")\n",
    "print(f\"Win rate against top 10: {best_player_named['win_percentage_vs_top10'].values[0]:.2f}%\")\n",
    "print('the number of opponents', len(opponents.drop_duplicates(subset=\"player_id\")))\n",
    "print(\"the number of winning opponents\", len(win_rate.drop_duplicates()))\n",
    "print(f'player info:\\n', players[players[\"player_id\"] == best_player_named['player_id'].values[0]].drop_duplicates(subset=\"player_id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- Best player against top 10: **Rocha, Francisco with ID: 228155**\n",
    "- He have played 2 matches against top 10 players and She won 1 of them.\n",
    "- Win rate against top 10: 50.00% with highest matchs number against top 10\n",
    "\n",
    "#### This statistic does not have a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- There are **40** players as competitors for the top 10 ranked players.\n",
    "- Among the **40** competitors, only **3** were able to win the game against the 10 highest ranked players.\n",
    "- And the best of them is undoubtedly **Rocha, Francisco with ID: 228155**\n",
    "- He is from **Portugal**, **185 cm tall**, weight unknown, current rank **934**.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### Question 17 ‚Äî What is the average number of breaks of serve per match?\n",
    "\n",
    "To calculate the average break of serve per match, we only need the data set `power_final.csv`.\n",
    "\n",
    "In this data, each row is for a game, there is a column called `break_occurred` that indicates whether a break occurred in that game.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading a dataset and saving it to a dataframe\n",
    "2. Extract `break_occurred`s by grouping based on `match_id`, `set_num`, and `game_num` to get the breaks for each game, and with the help of the `any` function, we can understand the result of having a break or not when dealing with duplicate data.\n",
    "3. Grouping breaks per game based on `match_id` and summing breaks per match\n",
    "4. Averaging games per match\n",
    "5. Filtering outlier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power = pd.read_csv(os.path.join(etl_files_path, \"power_final.csv\"))\n",
    "\n",
    "breaks_per_game = df_power.groupby(['match_id', 'set_num', 'game_num'])['break_occurred'].any()\n",
    "\n",
    "breaks_per_game = breaks_per_game.groupby('match_id').sum()\n",
    "\n",
    "total_avg = breaks_per_game.mean()\n",
    "\n",
    "breaks_per_game = breaks_per_game[(breaks_per_game <= 20) & (breaks_per_game > 0)]\n",
    "\n",
    "\n",
    "print(\"Average breaks per match (excluding outliers):\", total_avg)\n",
    "print(\"Max breaks in a match (for filtered data):\", breaks_per_game.max())\n",
    "print(\"Total matches considered:\", len(breaks_per_game))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- The average number of breaks per match is **7.59**\n",
    "\n",
    "#### This dataset does not have any NaN data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.histplot(\n",
    "            x=breaks_per_game,\n",
    "            bins=20,\n",
    "            kde=True,\n",
    "            ax=ax,\n",
    "            color='skyblue',\n",
    "            label='Breaks per Match',\n",
    "            edgecolor='gray',\n",
    "            alpha=0.7)\n",
    "\n",
    "ax.set_title('Distribution of Breaks per Match', loc='left', fontsize=16, color='black', pad=20)\n",
    "ax.set_xlabel('Number of Breaks', loc='left')\n",
    "ax.set_ylabel('Number of Matches', loc='bottom')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.legend(loc='upper left')\n",
    "ax.tick_params(axis='both', colors='gray', labelsize=10)\n",
    "ax.set_xticks(range(0, 21))\n",
    "ax.axvline(total_avg, color='red', linestyle='--', label='Mean Breaks per Match')\n",
    "ax.text(total_avg, ax.get_ylim()[1]*0.9, f'Mean: {total_avg:.2f}',\n",
    "        rotation=90, color='red', verticalalignment='top', horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- About 10,968 matches were analyzed in this statistic, with the highest number of breaks per match being 20 breaks per match.\n",
    "- A very small number of matches had 20 breaks per match, which were removed from the statistics as outlier data, and the data was kept up to 20 for the sake of the beauty of the distribution.\n",
    "\n",
    "--------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {

   "display_name": ".venv (3.13.5)",

   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",

   "version": "3.13.5"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
