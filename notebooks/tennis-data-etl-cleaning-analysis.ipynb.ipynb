{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cbb01a",
   "metadata": {},
   "source": [
    "# Data Reading Section:\n",
    "##### In this section, we will use the following codes, which will be explained below, to read the data we need from the main reference file and convert it to CSV format so that we can use it in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9223672",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f794cb9",
   "metadata": {},
   "source": [
    "# Part 1: Importing the required libraries, defining the paths, and creating the required directories if they do not exist.\n",
    "In this section, we import the libraries and items we need to use them later, and then we define the main paths, such as the main zip file path, the output file directory, and the temp directory, in a relational manner, to be included in the data folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be967c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "\n",
    "# Define paths\n",
    "main_zip = \"..\\\\data\\\\raw\\\\tennis_data.zip\"\n",
    "output_dir = \"..\\\\data\\\\processed\"\n",
    "temp_dir = \"..\\\\data\\\\raw\\\\temp\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c20e4",
   "metadata": {},
   "source": [
    "# Part 2: Creating a CSV table generator function from Parquet files\n",
    "In this section, we have created a very useful function that, based on the keyword of the parquet category name that we give it, goes to the defined path of the main zip file and reads the parquets belonging to the specified tables and the data related to the specified columns from the zip files for each day. In addition to all this, we specify that the records of this table should be unique based on the unique data identifier or that this table can have multiple rows for each unique identifier. Our unique identifier is match_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(table_keyword, needed_cols, output_name, dedup_on=\"match_id\"):\n",
    "    \"\"\"\n",
    "    table_keyword: like 'event_' or 'home_team_'\n",
    "    needed_cols: list of needed columns\n",
    "    output_name: name of output CSV file\n",
    "    dedup_on: unique column for deduplication (default is 'match_id')\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(output_dir, output_name)\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    all_dfs = []\n",
    "    row_counter = 0\n",
    "\n",
    "    with zipfile.ZipFile(main_zip, \"r\") as main_zip_ref:\n",
    "        daily_zips = main_zip_ref.namelist()\n",
    "        print(f\"üì¶ Count of daily zips: {len(daily_zips)}\")\n",
    "\n",
    "        for i, daily_zip_name in enumerate(daily_zips, start=1):\n",
    "            print(f\"üîπ ({i}/{len(daily_zips)}) processing {daily_zip_name} ...\")\n",
    "            main_zip_ref.extract(daily_zip_name, temp_dir)\n",
    "            daily_zip_path = os.path.join(temp_dir, daily_zip_name)\n",
    "\n",
    "            with zipfile.ZipFile(daily_zip_path, \"r\") as daily_zip_ref:\n",
    "                parquet_files = [f for f in daily_zip_ref.namelist() if f.endswith(\".parquet\") and table_keyword in f]\n",
    "                for f in parquet_files:\n",
    "                    with daily_zip_ref.open(f) as pf:\n",
    "                        table = pq.read_table(BytesIO(pf.read()))\n",
    "                        df = table.to_pandas()\n",
    "                        df = df[[c for c in needed_cols if c in df.columns]]\n",
    "                        df[\"date_source\"] = daily_zip_name.replace(\".zip\", \"\")\n",
    "                        all_dfs.append(df)\n",
    "                        row_counter += len(df)\n",
    "\n",
    "            os.remove(daily_zip_path)\n",
    "\n",
    "    if all_dfs:\n",
    "        df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Shape: {df_all.shape}\")\n",
    "        if dedup_on and dedup_on in df_all.columns:\n",
    "            df_all = df_all.drop_duplicates(subset=dedup_on)\n",
    "        else:\n",
    "            df_all = df_all.drop_duplicates()\n",
    "        print(f\"üßπ after cleaning duplicated rows: {df_all.shape}\")\n",
    "\n",
    "        df_all.to_csv(csv_path, index=False)\n",
    "        print(f\"üíæ Saved: {csv_path}\")\n",
    "        print(f\"üìä Count of all rows: {len(df_all)}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è There is no file for {table_keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97486bc8",
   "metadata": {},
   "source": [
    "# Part 3: Using the above cell function and creating CSVs of the tables required for analysis according to the columns required from them\n",
    "In this part, based on the initial analysis we had of the 17 questions in question and the data they required, we extracted a series of tables from a total of 15 tables and a series of their columns that were needed to analyze and answer the 17 questions we needed. Here, we want to extract them from the original raw zip file and convert them to CSV files so that we can use these files later in analyzing and answering the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594441d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_table(\n",
    "    table_keyword=\"event_\",\n",
    "    needed_cols=[\"match_id\", \"first_to_serve\", \"winner_code\", \"default_period_count\", \"start_datetime\", \"match_slug\"],\n",
    "    output_name=\"event.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"home_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"home_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"away_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"away_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"tournament_\",\n",
    "    needed_cols=[\"match_id\", \"tournament_id\", \"tournament_name\", \"ground_type\", \"tennis_points\", \"start_datetime\"],\n",
    "    output_name=\"tournament.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"time_\",\n",
    "    needed_cols=[\"match_id\", \"period_1\", \"period_2\", \"period_3\", \"period_4\", \"period_5\", \"current_period_start_timestamp\"],\n",
    "    output_name=\"time.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"statistics_\",\n",
    "    needed_cols=[\"match_id\", \"statistic_name\", \"home_value\", \"away_value\"],\n",
    "    output_name=\"statistics.csv\",\n",
    "    dedup_on=None  # No deduplication because we have multiple rows per match_id in statistics\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"power_\",\n",
    "    needed_cols=[\"match_id\", \"set_num\", \"game_num\", \"value\", \"break_occurred\"],\n",
    "    output_name=\"power.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in power\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"pbp_\",\n",
    "    needed_cols=[\"match_id\", \"set_id\", \"game_id\", \"point_id\", \"home_point\", \"away_point\"],\n",
    "    output_name=\"pbp.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in pbp\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
