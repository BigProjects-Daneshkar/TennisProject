{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Reading Section:\n",
    "##### In this section, we will use the following codes, which will be explained below, to read the data we need from the main reference file and convert it to CSV format so that we can use it in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è IMPORTANT ‚Äî READ BEFORE RUNNING\n",
    "\n",
    "This notebook expects the raw dataset to be available **before execution**.  \n",
    "If the required ZIP file is not placed in the correct path, the ETL pipeline will fail or generate incomplete/duplicated outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Required Action\n",
    "\n",
    "Please make sure the following file exists **before running the notebook**:\n",
    "\n",
    "..\\data\\raw\\tennis_data.zip\n",
    "\n",
    "\n",
    "> üìå The path is already configured inside the notebook‚Äôs Python extraction script ‚Äî do **not** change it unless necessary.\n",
    "\n",
    "If the ZIP file has a different name, please rename it or update the code accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 1: Importing the required libraries, defining the paths, and creating the required directories if they do not exist.\n",
    "In this section, we import the libraries and items we need to use them later, and then we define the main paths, such as the main zip file path, the output file directory, and the temp directory, in a relational manner, to be included in the data folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "# Define paths\n",
    "main_zip = r\".\\data\\raw\\tennis_data.zip\"\n",
    "output_dir = r\".\\data\\processed\"\n",
    "temp_dir = r\".\\data\\raw\\temp\"\n",
    "base_path = r\"..\\data\\processed\"\n",
    "clean_path = r\"..\\data\\processed\\clean\"\n",
    "etl_files_path = r\"..\\data\\processed\\clean\"\n",
    "\n",
    "os.makedirs(clean_path, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Part 2: Creating a CSV table generator function from Parquet files\n",
    "In this section, we have created a very useful function that, based on the keyword of the parquet category name that we give it, goes to the defined path of the main zip file and reads the parquets belonging to the specified tables and the data related to the specified columns from the zip files for each day. In addition to all this, we specify that the records of this table should be unique based on the unique data identifier or that this table can have multiple rows for each unique identifier. Our unique identifier is match_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(table_keyword, needed_cols, output_name, dedup_on=\"match_id\"):\n",
    "    \"\"\"\n",
    "    table_keyword: like 'event_' or 'home_team_'\n",
    "    needed_cols: list of needed columns\n",
    "    output_name: name of output CSV file\n",
    "    dedup_on: unique column for deduplication (default is 'match_id')\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(output_dir, output_name)\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    all_dfs = []\n",
    "    row_counter = 0\n",
    "\n",
    "    with zipfile.ZipFile(main_zip, \"r\") as main_zip_ref:\n",
    "        daily_zips = main_zip_ref.namelist()\n",
    "        print(f\"üì¶ Count of daily zips: {len(daily_zips)}\")\n",
    "\n",
    "        for i, daily_zip_name in enumerate(daily_zips, start=1):\n",
    "            print(f\"üîπ ({i}/{len(daily_zips)}) processing {daily_zip_name} ...\")\n",
    "            main_zip_ref.extract(daily_zip_name, temp_dir)\n",
    "            daily_zip_path = os.path.join(temp_dir, daily_zip_name)\n",
    "\n",
    "            with zipfile.ZipFile(daily_zip_path, \"r\") as daily_zip_ref:\n",
    "                parquet_files = [f for f in daily_zip_ref.namelist() if f.endswith(\".parquet\") and table_keyword in f]\n",
    "                for f in parquet_files:\n",
    "                    with daily_zip_ref.open(f) as pf:\n",
    "                        table = pq.read_table(BytesIO(pf.read()))\n",
    "                        df = table.to_pandas()\n",
    "                        df = df[[c for c in needed_cols if c in df.columns]]\n",
    "                        df[\"date_source\"] = daily_zip_name.replace(\".zip\", \"\")\n",
    "                        all_dfs.append(df)\n",
    "                        row_counter += len(df)\n",
    "\n",
    "            os.remove(daily_zip_path)\n",
    "\n",
    "    if all_dfs:\n",
    "        df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Shape: {df_all.shape}\")\n",
    "        if dedup_on and dedup_on in df_all.columns:\n",
    "            df_all = df_all.drop_duplicates(subset=dedup_on)\n",
    "        else:\n",
    "            df_all = df_all.drop_duplicates()\n",
    "        print(f\"üßπ after cleaning duplicated rows: {df_all.shape}\")\n",
    "\n",
    "        df_all.to_csv(csv_path, index=False)\n",
    "        print(f\"üíæ Saved: {csv_path}\")\n",
    "        print(f\"üìä Count of all rows: {len(df_all)}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è There is no file for {table_keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Part 3: Using the above cell function and creating CSVs of the tables required for analysis according to the columns required from them\n",
    "In this part, based on the initial analysis we had of the 17 questions in question and the data they required, we extracted a series of tables from a total of 15 tables and a series of their columns that were needed to analyze and answer the 17 questions we needed. Here, we want to extract them from the original raw zip file and convert them to CSV files so that we can use these files later in analyzing and answering the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_table(\n",
    "    table_keyword=\"event_\",\n",
    "    needed_cols=[\"match_id\", \"first_to_serve\", \"winner_code\", \"default_period_count\", \"start_datetime\", \"match_slug\"],\n",
    "    output_name=\"event.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"home_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"home_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"away_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"away_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"tournament_\",\n",
    "    needed_cols=[\"match_id\", \"tournament_id\", \"tournament_name\", \"ground_type\", \"tennis_points\", \"start_datetime\"],\n",
    "    output_name=\"tournament.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"time_\",\n",
    "    needed_cols=[\"match_id\", \"period_1\", \"period_2\", \"period_3\", \"period_4\", \"period_5\", \"current_period_start_timestamp\"],\n",
    "    output_name=\"time.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"statistics_\",\n",
    "    needed_cols=[\"match_id\", \"statistic_name\", \"home_value\", \"away_value\"],\n",
    "    output_name=\"statistics.csv\",\n",
    "    dedup_on=None  # No deduplication because we have multiple rows per match_id in statistics\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"power_\",\n",
    "    needed_cols=[\"match_id\", \"set_num\", \"game_num\", \"value\", \"break_occurred\"],\n",
    "    output_name=\"power.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in power\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"pbp_\",\n",
    "    needed_cols=[\"match_id\", \"set_id\", \"game_id\", \"point_id\", \"home_point\", \"away_point\", \"home_score\"],\n",
    "    output_name=\"pbp.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in pbp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "##  Part 4: Data Cleaning Stage\n",
    "#### In this section, we will clean the extracted CSV files created in the previous section.\n",
    "# \n",
    "### **Goal:**  \n",
    "- Remove duplicate rows  \n",
    " - Handle missing values (`NaN`)  \n",
    " - Standardize data types  \n",
    " The cleaned outputs will be stored in `../data/clean` for the next normalization phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "###  Cleaning: Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(os.path.join(base_path, \"event.csv\"))\n",
    "df_event.drop_duplicates(inplace=True)\n",
    "\n",
    "for col in df_event.columns:\n",
    "    if df_event[col].dtype == 'object':\n",
    "        df_event[col] = df_event[col].fillna(\"Unknown\")\n",
    "    else:\n",
    "        df_event[col] = df_event[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_event.columns:\n",
    "    df_event[\"match_id\"] = df_event[\"match_id\"].astype(str)\n",
    "\n",
    "df_event.to_csv(os.path.join(clean_path, \"event_clean.csv\"), index=False)\n",
    "print(\"‚úÖ event_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "###  Cleaning: Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(base_path, \"home_team.csv\"))\n",
    "df_home = df_home.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(str)\n",
    "\n",
    "df_home.to_csv(os.path.join(clean_path, \"home_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ home_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "###  Cleaning: Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_away = pd.read_csv(os.path.join(base_path, \"away_team.csv\"))\n",
    "df_away = df_away.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(str)\n",
    "\n",
    "df_away.to_csv(os.path.join(clean_path, \"away_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ away_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Cleaning: Tournamet Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tournament = pd.read_csv(os.path.join(base_path, \"tournament.csv\"))\n",
    "df_tournament = df_tournament.drop_duplicates()\n",
    "\n",
    "mask = df_tournament['ground_type'].isnull() | (df_tournament['ground_type'].str.strip() == '')\n",
    "df_tournament.loc[mask, 'ground_type'] = 'Unknown'\n",
    "\n",
    "df_tournament.to_csv(os.path.join(clean_path, \"tournament_clean.csv\"), index=False)\n",
    "print(\"‚úÖ tournament_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Cleaning: Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistics = pd.read_csv(os.path.join(base_path, \"statistics.csv\"))\n",
    "df_statistics = df_statistics.drop_duplicates()\n",
    "\n",
    "# This data dosent have any nulls to clean however we display the count of nulls for verification\n",
    "display(df_statistics.isnull().sum())\n",
    "\n",
    "if \"match_id\" in df_statistics.columns:\n",
    "    df_statistics[\"match_id\"] = df_statistics[\"match_id\"].astype(str)\n",
    "\n",
    "df_statistics.to_csv(os.path.join(clean_path, \"statistics_clean.csv\"), index=False)\n",
    "print(\"‚úÖ statistics_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Cleaning: Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.read_csv(os.path.join(base_path, \"time.csv\"))\n",
    "df_time = df_time.drop_duplicates()\n",
    "\n",
    "df_time.drop(columns=[\"period_4\", \"period_5\"], inplace=True) # because all tennis matches are best of 3 sets\n",
    "\n",
    "if \"match_id\" in df_time.columns:\n",
    "    df_time[\"match_id\"] = df_time[\"match_id\"].astype(int)\n",
    "\n",
    "df_time.to_csv(os.path.join(clean_path, \"time_clean.csv\"), index=False)\n",
    "print(\"‚úÖ time_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Cleaning: Point By Point Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pbp = pd.read_csv(os.path.join(base_path, \"pbp.csv\"))\n",
    "df_pbp = df_pbp.drop_duplicates()\n",
    "\n",
    "df_pbp['home_point'] = df_pbp['home_point'].replace('A', 1).astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].replace('A', 1).astype(int)\n",
    "\n",
    "# This data dosent have any nulls to clean however we display the count of nulls for verification\n",
    "display(df_pbp.isnull().sum())\n",
    "\n",
    "if \"match_id\" in df_pbp.columns:\n",
    "    df_pbp[\"match_id\"] = df_pbp[\"match_id\"].astype(str)\n",
    "\n",
    "\n",
    "df_pbp.to_csv(os.path.join(clean_path, \"pbp_clean.csv\"), index=False)\n",
    "print(\"‚úÖ pbp_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Cleaning: Power Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power = pd.read_csv(os.path.join(base_path, \"power.csv\"))\n",
    "df_power = df_power.drop_duplicates()\n",
    "\n",
    "# No nulls to clean, just save the cleaned file and other column is clean as you can see here\n",
    "display(\"Sum of nulls:\", df_power.isnull().sum())\n",
    "display(\"Sum of invalid game_num entries:\", df_power['match_id'][df_power['game_num'] < 1].sum())\n",
    "display(\"DataFrame dtypes for verification correctness of data:\", df_power.dtypes)\n",
    "\n",
    "df_power.to_csv(os.path.join(clean_path, \"power_clean.csv\"), index=False)\n",
    "print(\"‚úÖ power_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Part 5: Normalization Stage\n",
    "#### Now that we have clean CSVs, in this part we will:\n",
    "#\n",
    " - Convert data types (e.g., timestamps to datetime)  \n",
    " - Standardize text (e.g., capitalization, spacing)  \n",
    " - Fill remaining missing values intelligently (using mean, median, or mode)  \n",
    " The normalized final datasets will be saved in `../data/processed/clean` as `_final.csv` files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"event_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"event_final.csv\")\n",
    "\n",
    "df_event = pd.read_csv(input_path)\n",
    "\n",
    "df_event[\"match_id\"] = df_event[\"match_id\"].astype(int)\n",
    "df_event[\"default_period_count\"] = df_event[\"default_period_count\"].astype(int)\n",
    "df_event[\"date_source\"] = df_event[\"date_source\"].astype(int)\n",
    "\n",
    "if np.issubdtype(df_event[\"start_datetime\"].dtype, np.number):\n",
    "    df_event[\"start_datetime\"] = pd.to_datetime(df_event[\"start_datetime\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "df_event[\"winner_code\"] = df_event[\"winner_code\"].fillna(df_event[\"winner_code\"].mode()[0])\n",
    "df_event[\"first_to_serve\"] = df_event[\"first_to_serve\"].fillna(df_event[\"first_to_serve\"].mode()[0])\n",
    "\n",
    "df_event.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ event_final.csv created successfully!\")\n",
    "print(df_event.info())\n",
    "print(df_event.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"home_team_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"home_team_final.csv\")\n",
    "\n",
    "df_home = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = pd.to_numeric(df_home[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_home.columns:\n",
    "    df_home[\"gender\"] = df_home[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_home.columns:\n",
    "    df_home[\"plays\"] = df_home[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_home.columns:\n",
    "    df_home[\"height\"] = df_home[\"height\"].fillna(df_home[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_home.columns:\n",
    "    df_home[\"weight\"] = df_home[\"weight\"].fillna(df_home[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_home.columns:\n",
    "    df_home[\"current_rank\"] = df_home[\"current_rank\"].fillna(df_home[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_home.columns:\n",
    "        mode_val = df_home[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_home[col] = df_home[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(int)\n",
    "\n",
    "df_home.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ home_team_final.csv created successfully!\")\n",
    "print(df_home.info())\n",
    "print(df_home.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"away_team_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"away_team_final.csv\")\n",
    "\n",
    "df_away = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = pd.to_numeric(df_away[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_away.columns:\n",
    "    df_away[\"gender\"] = df_away[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_away.columns:\n",
    "    df_away[\"plays\"] = df_away[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_away.columns:\n",
    "    df_away[\"height\"] = df_away[\"height\"].fillna(df_away[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_away.columns:\n",
    "    df_away[\"weight\"] = df_away[\"weight\"].fillna(df_away[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_away.columns:\n",
    "    df_away[\"current_rank\"] = df_away[\"current_rank\"].fillna(df_away[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_away.columns:\n",
    "        mode_val = df_away[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_away[col] = df_away[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(int)\n",
    "\n",
    "df_away.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ away_team_final.csv created successfully!\")\n",
    "print(df_away.info())\n",
    "print(df_away.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Tournament Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"tournament_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"tournament_final.csv\")\n",
    "\n",
    "df_tournament = pd.read_csv(input_path)\n",
    "\n",
    "df_tournament['match_id'] = df_tournament['match_id'].astype(int)\n",
    "\n",
    "df_tournament.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ tournament_final.csv created successfully!\")\n",
    "print(df_tournament.info())\n",
    "print(df_tournament.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"statistics_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"statistics_final.csv\")\n",
    "\n",
    "df_statistics = pd.read_csv(input_path)\n",
    "\n",
    "df_statistics['date_source'] = pd.to_datetime(df_statistics['date_source'], format='%Y%m%d')\n",
    "df_statistics['home_value'] = pd.to_numeric(df_statistics['home_value'], errors='coerce')\n",
    "df_statistics['away_value'] = pd.to_numeric(df_statistics['away_value'], errors='coerce')\n",
    "df_statistics['statistic_name'] = df_statistics['statistic_name'].astype(str).str.replace(\" \", \"_\").str.lower()\n",
    "df_statistics['match_id'] = df_statistics['match_id'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "df_statistics.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ statistics_final.csv created successfully!\")\n",
    "print(df_statistics.info())\n",
    "print(df_statistics.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"time_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"time_final.csv\")\n",
    "\n",
    "df_time = pd.read_csv(input_path)\n",
    "\n",
    "periods = [\"period_1\", \"period_2\", \"period_3\"]\n",
    "\n",
    "df_time[\"match_id\"] = df_time[\"match_id\"].astype(int)\n",
    "df_time['date_source'] = pd.to_datetime(df_time['date_source'], format='%Y%m%d')\n",
    "df_time['current_period_start_timestamp'] = pd.to_datetime(df_time['current_period_start_timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "df_time['match_id'] = df_time['match_id'].astype(int)\n",
    "\n",
    "MS_TRESHOLD = 100_000  # 100,000 milliseconds = 100 seconds\n",
    "\n",
    "for period in periods:\n",
    "    df_time[period] = pd.to_numeric(df_time[period], errors='coerce').abs()\n",
    "    mask = df_time[period] > MS_TRESHOLD\n",
    "    df_time.loc[mask, period] = df_time.loc[mask, period] / 1000 # convert milliseconds to seconds\n",
    "\n",
    "\n",
    "df_time.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ time_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Point By Point Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"pbp_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"pbp_final.csv\")\n",
    "\n",
    "df_pbp = pd.read_csv(input_path)\n",
    "\n",
    "df_pbp['date_source'] = pd.to_datetime(df_pbp['date_source'], format='%Y%m%d')\n",
    "df_pbp['match_id'] = df_pbp['match_id'].astype(int)\n",
    "df_pbp['home_point'] = df_pbp['home_point'].astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].astype(int)\n",
    "df_pbp['home_score'] = df_pbp['home_score'].astype(int)\n",
    "df_pbp['set_id'] = df_pbp['set_id'].astype(int)\n",
    "df_pbp['game_id'] = df_pbp['game_id'].astype(int)\n",
    "df_pbp['point_id'] = df_pbp['point_id'].astype(int)\n",
    "\n",
    "df_pbp.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ pbp_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Normalization ‚Äî Power Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(clean_path, \"pbp_clean.csv\")\n",
    "output_path = os.path.join(clean_path, \"pbp_final.csv\")\n",
    "\n",
    "df_pbp = pd.read_csv(input_path)\n",
    "\n",
    "df_pbp['date_source'] = pd.to_datetime(df_pbp['date_source'], format='%Y%m%d')\n",
    "df_pbp['match_id'] = df_pbp['match_id'].astype(int)\n",
    "df_pbp['home_point'] = df_pbp['home_point'].astype(int)\n",
    "df_pbp['away_point'] = df_pbp['away_point'].astype(int)\n",
    "df_pbp['set_id'] = df_pbp['set_id'].astype(int)\n",
    "df_pbp['game_id'] = df_pbp['game_id'].astype(int)\n",
    "df_pbp['point_id'] = df_pbp['point_id'].astype(int)\n",
    "\n",
    "df_pbp.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ pbp_final.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Question 1 ‚Äî How many tennis players are included in the dataset?\n",
    "\n",
    "To find the number of unique tennis players, we combine the home and away team tables, remove duplicate players based on `player_id`, and count how many unique players remain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "# Load cleaned & normalized player data\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# Combine home and away players\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# Keep only unique players\n",
    "unique_players = players.drop_duplicates(subset=\"player_id\")\n",
    "\n",
    "# Count unique players\n",
    "total_unique_players = unique_players.shape[0]\n",
    "\n",
    "print(\"total unique players =\",total_unique_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "###  Question 2 ‚Äî What is the average height of the players?\n",
    "The goal of this question is to calculate the average height of the tennis players in the dataset.\n",
    "To do this, we first need to create a unique players table so that players who are duplicates in home and away are not counted again.\n",
    "Then we correct invalid heights (such as 0 or NaN) and calculate the true average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "# load data\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# combine & remove duplicate players\n",
    "players = pd.concat([df_home , df_away], ignore_index=True)\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\").copy()\n",
    "\n",
    "# replace zeros with NaN\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].replace(0, np.nan)\n",
    "\n",
    "# fill missing heights with mean\n",
    "mean_height = players_unique[\"height\"].mean(skipna=True)\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].fillna(mean_height)\n",
    "\n",
    "# Calculate average height\n",
    "average_height = players_unique[\"height\"].mean()\n",
    "print(\"Average height =\" , average_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The histogram + KDE curve helps us observe:  \n",
    "- The central height tendency (mean around ~182 cm)  \n",
    "- Spread of heights  \n",
    "- Possible outliers  \n",
    "- Whether the distribution is normal or skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Player Height Distribution\")\n",
    "sns.histplot(players_unique[\"height\"], kde=True, bins=30)\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Insight\n",
    "The average tennis player height is around **182 cm**.  \n",
    "The distribution appears slightly right-skewed, meaning a small number of players are significantly taller than the average.\n",
    "\n",
    "Most players fall between **175‚Äì190 cm**, which aligns with typical professional tennis standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Question 10 ‚Äî Correlation Between Player Height and Ranking\n",
    "Interpretation\n",
    "\n",
    "We want to check whether taller players tend to have higher or lower rankings.\n",
    "We combine home & away players, remove duplicates, clean invalid values, and compute Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "# Load datasets\n",
    "df_home = pd.read_csv(os.path.join(base, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(base, \"away_team_final.csv\"))\n",
    "\n",
    "# Merge home + away\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# Keep unique players\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\")\n",
    "\n",
    "# Replace invalid zeros with NaN\n",
    "players_unique.loc[:, \"height\"] = players_unique[\"height\"].replace(0, np.nan)\n",
    "players_unique.loc[:, \"current_rank\"] = players_unique[\"current_rank\"].replace(0, np.nan)\n",
    "\n",
    "# Drop rows with missing required values\n",
    "clean_players = players_unique.dropna(subset=[\"height\", \"current_rank\"])\n",
    "\n",
    "# Compute correlation\n",
    "correlation = clean_players[\"height\"].corr(clean_players[\"current_rank\"], method=\"pearson\")\n",
    "\n",
    "print(\"correlation =\" , correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Correlation = 0.10355\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "There is no meaningful correlation between player height and ranking.\n",
    "Height does not significantly influence global ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(\n",
    "    x=\"height\",\n",
    "    y=\"current_rank\",\n",
    "    data=clean_players,\n",
    "    scatter_kws={\"alpha\":0.4},\n",
    "    line_kws={\"color\":\"red\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Height vs Ranking\")\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Ranking (Lower is Better)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "###  Insight\n",
    "The correlation coefficient was approximately **0.10**, indicating a **very weak positive correlation**.\n",
    "\n",
    "This means **taller players tend to have slightly worse rankings**, but the effect is extremely small.  \n",
    "\n",
    "Height does NOT strongly predict performance or ranking in professional tennis.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Question 11 ‚Äî What is the average duration of matches?\n",
    "\n",
    "To calculate the average match duration, we used the `time_final.csv` dataset, which contains\n",
    "the duration of each period inside a tennis match:\n",
    "\n",
    "- `period_1`\n",
    "- `period_2`\n",
    "- `period_3`\n",
    "\n",
    "These periods contain the **duration in seconds**.\n",
    "\n",
    "#### **Steps**\n",
    "1. Replace NaN values in period columns with 0.  \n",
    "2. Compute total match duration:  `duration_seconds = period_1 + period_2 + period_3`  \n",
    "3. Keep only matches where duration > 0.  \n",
    "4. Compute the mean duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/macbook/Downloads/Daneshkar/tennis project/TennisProject/data/processed/clean\"\n",
    "\n",
    "df_time = pd.read_csv(os.path.join(base, \"time_final.csv\"))\n",
    "\n",
    "# Replace NaN periods with 0\n",
    "for col in [\"period_1\", \"period_2\", \"period_3\"]:\n",
    "    df_time[col] = df_time[col].fillna(0)\n",
    "\n",
    "# Compute duration in seconds\n",
    "df_time.loc[:, \"duration_seconds\"] = df_time[\"period_1\"] + df_time[\"period_2\"] + df_time[\"period_3\"]\n",
    "\n",
    "# Filter out zeros (invalid matches)\n",
    "df_valid = df_time[df_time[\"duration_seconds\"] > 0].copy()\n",
    "df_valid[\"duration_minutes\"] = df_valid[\"duration_seconds\"] / 60\n",
    "\n",
    "# Average duration\n",
    "avg_sec = df_valid[\"duration_seconds\"].mean()\n",
    "avg_minutes = avg_sec / 60\n",
    "avg_hours = avg_minutes / 60\n",
    "\n",
    "print(\"Average duration (seconds)=\", avg_sec)\n",
    "print(\"Average duration (minutes)=\", avg_minutes)\n",
    "print(\"Average duration (hours)=\", avg_hours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- **Average duration (seconds):** 6705.87  \n",
    "- **Average duration (minutes):** 111.76  \n",
    "- **Average duration (hours):** 1.86  \n",
    "\n",
    "#### **Average tennis match duration ‚âà 1 hour and 52 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Distribution of Match Duration\")\n",
    "sns.histplot(df_valid[\"duration_minutes\"], kde=True, bins=40)\n",
    "plt.xlabel(\"Duration (minutes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "###  Insight\n",
    "\n",
    "The distribution shows:  \n",
    "- Most matches are between **60 and 130 minutes**  \n",
    "- A small number of matches last more than **180 minutes**  \n",
    "- Very short or extremely long matches are rare  \n",
    "  \n",
    "  --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Question 12 - What is the average number of games per set in men's matches compared to women's matches?\n",
    "\n",
    "To calculate the average number of games per set for men's and women's matches, we used three main datasets: `home_team_final.csv`, `away_team_final.csv`, and `pbp_final.csv`.\n",
    "\n",
    "The `gender` column in the `home_team_final.csv` and `away_team_final.csv` datasets contains information about the **gender** of the players, and in the `pbp_final.csv` dataset we can also have `information about each game` to count it into women's and men's matches.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading datasets and storing them in code as dataframes\n",
    "2. Create a `gender` dataframe that contains information about the gender and matches played by home and away players.\n",
    "3. Obtaining the gender of each match using player grouping and getting the mode of the players for each match with the code `df_gender = df_gender.groupby(\"match_id\")['gender'].apply(lambda x: x.mode()[0]).reset_index()`\n",
    "4. Counting games per match and storing them in `df_games`\n",
    "5. Merging the `df_gender` and `df_games` dataframes for the final calculation\n",
    "6. Obtain the final result by grouping `df_merged` by gender and averaging for each gender (ignoring data where the `gender` column is unknown)\n",
    "7. Obtaining the ratio of gender-unknown data to total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\"))\n",
    "df_away = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\"))\n",
    "df_pbp = pd.read_csv(os.path.join(etl_files_path, \"pbp_final.csv\"))\n",
    "\n",
    "df_gender = pd.concat(\n",
    "    [df_home[['match_id', 'gender']],\n",
    "     df_away[['match_id', 'gender']]],\n",
    "    ignore_index=True)\n",
    "\n",
    "df_gender = df_gender.groupby(\"match_id\")['gender'].apply(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "df_games = df_pbp.groupby(['match_id', 'set_id',])['game_id'].nunique().reset_index(name='games_in_set')\n",
    "\n",
    "df_merged = df_games.merge(df_gender, on='match_id', how='inner')\n",
    "\n",
    "result = df_merged.groupby('gender')['games_in_set'].mean().drop('Unknown')\n",
    "\n",
    "stat = df_merged.groupby('gender')['games_in_set']\n",
    "\n",
    "ratio_of_unknown = (df_merged['gender'] == 'Unknown').sum() / len(df_merged) * 100\n",
    "\n",
    "print(result)\n",
    "print(ratio_of_unknown)\n",
    "print('minimum games in a match', stat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- The average number of games per set in men's matches is approximately equal to **9.27**\n",
    "- The average number of games per set in women's matches is approximately equal to **8.90**\n",
    "- The highest number of games in a match for both women and men was **13**.\n",
    "\n",
    "#### The proportion of players whose gender was unknown to the total dataset is approximately equal to **5.1%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.barplot(\n",
    "    y=['Male', 'Female'],\n",
    "    x=result.values,\n",
    "    ax=ax,\n",
    "    color='skyblue',\n",
    "    hue=result.index,\n",
    "    edgecolor = 'gray',\n",
    "    palette=['skyblue', 'lightpink'],\n",
    "    alpha = 0.9\n",
    ")\n",
    "ax.set_title('Average Number of Games per Match by Gender', fontsize=16, loc='left')\n",
    "ax.set_ylabel('Gender', fontsize=14, loc='bottom', color='gray')\n",
    "ax.set_xlabel('Games per Match', fontsize=14, loc=\"left\", color='gray')\n",
    "ax.tick_params(axis='both', which='major', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "ax.set_xticks(np.arange(0, 10, 1))\n",
    "ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- The average number of games in a match between women and men is not much different **(around 0.3 games per match)**.\n",
    "- About **5.1%** of players had their gender unknown in the dataset and were not included in this statistic.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Question 13 ‚Äî What is the distribution of left-handed versus right-handed players?\n",
    "\n",
    "To calculate the distribution of left-handed versus right-handed players, we use two datasets, `home_team_final.csv` and `away_team_final.csv`, which contain player information.\n",
    "\n",
    "In both of these datasets, there is a column called `plays` that stores information about whether the player is left-handed or right-handed.\n",
    "\n",
    "#### **Steps**\n",
    "1. Read the dataset and store it in code as data frames, uniquely considering each player and extracting only the plays column.\n",
    "2. Creating df_all, which is a dataframe containing information about all players.\n",
    "3. Create a distribution dataframe that counts and stores data for left-handed or right-handed players or for players whose status is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\")).drop_duplicates(subset='player_id')['plays']\n",
    "df_away = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\")).drop_duplicates(subset='player_id')['plays']\n",
    "\n",
    "df_all = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "df_clean = df_all[df_all != 'unknown']\n",
    "\n",
    "distribution = df_all.value_counts(dropna=True)\n",
    "print(distribution)\n",
    "\n",
    "ratio_of_unknown = (df_all == 'unknown').sum() / len(df_all) * 100\n",
    "print('Ratio of unknown plays:', ratio_of_unknown)\n",
    "\n",
    "distribution = distribution.drop('unknown', errors='ignore')\n",
    "\n",
    "ratio_of_right_handed_without_unknown = (df_clean == 'right-handed').sum() / len(df_clean) * 100\n",
    "print('Ratio of right-handed players:', ratio_of_right_handed_without_unknown)\n",
    "\n",
    "ratio_of_left_handed_without_unknown = (df_clean == 'left-handed').sum() / len(df_clean) * 100\n",
    "print('Ratio of left-handed players:', ratio_of_left_handed_without_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- Without taking into account uncertain data, approximately **88.54% of players are right-handed**\n",
    "-  and **11.45% of them are left-handed**.\n",
    "\n",
    "#### Unfortunately, about **55.94%** of all players have data regarding their right- or left-handedness unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    y=distribution.index,\n",
    "    x=distribution.values,\n",
    "    palette=['skyblue', 'lightgreen'],\n",
    "    hue=distribution.index,\n",
    "    alpha=0.9,\n",
    "    edgecolor = 'gray'\n",
    "    )\n",
    "ax.set_title('Distribution of left-handed versus right-handed players', fontsize=16, loc='left')\n",
    "ax.set_ylabel('Handedness', fontsize=14, loc='bottom', color='gray')\n",
    "ax.set_xlabel('Number of Players', fontsize=14, loc=\"left\", color='gray')\n",
    "ax.tick_params(axis='both', which='major', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- Of all players, the left-handed or right-handed status of **2,749** is unknown, with **1,917** being right-handed and **248** being left-handed.\n",
    "- The total number of players considered in this section was **4914**\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Question 14 ‚Äî What is the most common type of surface used in tournaments?\n",
    "\n",
    "To calculate the most common field type in tournaments, we only need the `tournament_final.csv` dataset.\n",
    "\n",
    "In this dataset, there is a column called `ground_type` through which you can find out the type of ground on which the matches were held.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading a dataset and storing it in a dataframe\n",
    "2. Counting `ground_type`s in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tournaments = pd.read_csv(os.path.join(etl_files_path, \"tournament_final.csv\"))\n",
    "\n",
    "tournament_groundtype_counts = df_tournaments['ground_type'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "print(f'The most common ground type is: {tournament_groundtype_counts.idxmax()} with {tournament_groundtype_counts.max()} tournaments.')\n",
    "\n",
    "ratio_of_unknown = (df_tournaments['ground_type'] == 'Unknown').sum() / len(df_tournaments) * 100\n",
    "print('Ratio of unknown ground types:', ratio_of_unknown)\n",
    "print(tournament_groundtype_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- The most common ground type is: Hardcourt outdoor\n",
    "- Hardcourt outdoor has been used as a playing surface in 8116 tournaments\n",
    "\n",
    "#### Only 1.61% of the data had an unknown playing surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    y=tournament_groundtype_counts.index,\n",
    "    x=tournament_groundtype_counts.values,\n",
    "    palette=['skyblue', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray'],\n",
    "    hue=tournament_groundtype_counts.index,\n",
    "    edgecolor='gray',\n",
    "    alpha = 0.9\n",
    "    )\n",
    "ax.set_title('Most common playing surfaces', fontsize=16, loc='left')\n",
    "ax.set_ylabel('Playing Surface', fontsize=14, loc='bottom', color='gray')\n",
    "ax.set_xlabel('Number of Tournaments', fontsize=14, loc=\"left\", color='gray')\n",
    "ax.tick_params(axis='both', which='major', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- There are about **16,873** tournaments in the dataset, with the most common playing surface among them being **Hardcourt outdoor**, which was present as the playing surface in **8,116** tournaments.\n",
    "- The least popular playing surface among tournaments is **Green clay**, which has only been used as a playing surface in **18** tournaments.\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### Question 15 ‚Äî How many distinct countries are represented in the dataset?\n",
    "\n",
    "To calculate the individual countries in the dataset, we need to refer back to the `home_team_final.csv` and `away_team_final`.csv datasets.\n",
    "\n",
    "In both of these datasets, there is a column called `country` for each player, which specifies the player's nationality and shows how many distinct countries there are in our data.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading datasets and storing only their `country` column in dataframes\n",
    "2. Creating `df_country` which is a merged version of home and away countries\n",
    "3. Counting distinct countries in `df_country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\"))[\"country\"]\n",
    "df_away = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\"))[\"country\"]\n",
    "\n",
    "df_country = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "country_counts = df_country.value_counts().sort_values(ascending=False)\n",
    "\n",
    "country_counts_for_display = country_counts.head(10).drop(labels=[\"Unknown\"], errors='ignore')\n",
    "\n",
    "ratio_of_unknown = (df_country == 'Unknown').sum() / len(df_country) * 100\n",
    "print('Ratio of unknown countries:', ratio_of_unknown)\n",
    "print(country_counts.info())\n",
    "print(country_counts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- There are **101 distinct countries** in our dataset, excluding unknown data.\n",
    "- **France** has the highest number of players in our dataset.\n",
    "\n",
    "#### About 29.72% of the data related to countries is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.barplot(ax=ax,\n",
    "            x=country_counts_for_display.index,\n",
    "            y=country_counts_for_display.values,\n",
    "            hue=country_counts_for_display.index,\n",
    "            edgecolor='gray',\n",
    "            palette=['skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue', 'skyblue'],\n",
    "            alpha=0.9)\n",
    "ax.set_title(f'Top 10 Countries of Teams\\nRatio of \"Unknown\" countries: {ratio_of_unknown:.2f}%\\nThere are 101 distinct countries in the dataset.', loc='left', fontsize=16, color='gray', pad=20)\n",
    "ax.set_xlabel('Country', loc='left')\n",
    "ax.set_ylabel('Number of Teams', loc='bottom')\n",
    "ax.tick_params(axis='both', colors='gray', labelsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- Of the 101 distinct countries, **France** had the highest number of players in the dataset.\n",
    "- **Qatar**, **Bahamas** and **Haiti**, which are at the bottom of the table, each have only **1** player in the dataset.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "### Question 16 ‚Äî Which player has the highest winning percentage against top 10 ranked opponents?\n",
    "\n",
    "To find the player with the highest win rate against the 10 highest ranked players in the dataset, we need to use the datasets `home_team_final.csv`, `away_team_final.csv`, and `event_final.csv`.\n",
    "\n",
    "To access player information, we use the home and away datasets, and to access match information, especially match winner information, we use the `winner_code` column event dataset.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading datasets and saving them in dataframes\n",
    "2. Creating a `players` Table by Merging Home and Away\n",
    "3. Extracting the top 10 players based on rank using the `current_rank` column\n",
    "4. Extract all matches played by the top 10 players\n",
    "5. Extracting top 10 competitors in matches alongside themselves\n",
    "6. Merge the result with the event table to have the winner_code and the desired players in one place.\n",
    "7. Determine if each player won their match\n",
    "8. Filter NON‚ÄìTop10 players only (the opponents)\n",
    "9. Calculate win percentage vs Top10\n",
    "10. Find the best player (sort by win rate and number pf matchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home  = pd.read_csv(os.path.join(etl_files_path, \"home_team_final.csv\"))\n",
    "df_away  = pd.read_csv(os.path.join(etl_files_path, \"away_team_final.csv\"))\n",
    "df_event = pd.read_csv(os.path.join(etl_files_path, \"event_final.csv\"))\n",
    "\n",
    "players = pd.concat([df_home, df_away], ignore_index=True)\n",
    "\n",
    "# keep valid players only (remove rows with Unknown or rank=0)\n",
    "players = players[(players[\"player_id\"] != \"Unknown\") & (players[\"current_rank\"] > 0)].copy()\n",
    "\n",
    "# Create 'side' column: home=1, away=2\n",
    "home_ids = df_home[\"player_id\"].tolist()\n",
    "players[\"side\"] = players[\"player_id\"].apply(lambda x: 1 if x in home_ids else 2)\n",
    "\n",
    "players_unique = players.drop_duplicates(subset=\"player_id\")\n",
    "top10 = players_unique.sort_values(\"current_rank\", ascending=False).head(10)\n",
    "\n",
    "top10_ids = top10[\"player_id\"].tolist()\n",
    "\n",
    "matches_with_top10 = players[players[\"player_id\"].isin(top10_ids)][\"match_id\"].unique()\n",
    "\n",
    "subset_matches = players[players[\"match_id\"].isin(matches_with_top10)].copy()\n",
    "\n",
    "subset_matches = subset_matches.merge(\n",
    "    df_event[[\"match_id\", \"winner_code\"]],\n",
    "    on=\"match_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "subset_matches[\"is_winner\"] = (\n",
    "    ((subset_matches[\"side\"] == 1) & (subset_matches[\"winner_code\"] == 1)) |\n",
    "    ((subset_matches[\"side\"] == 2) & (subset_matches[\"winner_code\"] == 2))\n",
    ")\n",
    "\n",
    "opponents = subset_matches[~subset_matches[\"player_id\"].isin(top10_ids)].copy()\n",
    "\n",
    "win_rate = opponents.groupby(\"player_id\").agg(\n",
    "    matches_vs_top10=(\"match_id\", \"count\"),\n",
    "    wins_vs_top10=(\"is_winner\", \"sum\")\n",
    ")\n",
    "\n",
    "win_rate[\"win_percentage_vs_top10\"] = (\n",
    "    win_rate[\"wins_vs_top10\"] / win_rate[\"matches_vs_top10\"]\n",
    ") * 100\n",
    "\n",
    "# Add minimum matches threshold (recommended >= 5)\n",
    "threshold = 5\n",
    "\n",
    "win_rate_filtered = win_rate[win_rate[\"matches_vs_top10\"] >= threshold]\n",
    "\n",
    "# If no player passes threshold, fall back to >=2\n",
    "if len(win_rate_filtered) == 0:\n",
    "    win_rate_filtered = win_rate[win_rate[\"matches_vs_top10\"] >= 2]\n",
    "\n",
    "# Filter players that played minimum 2 matches vs top10\n",
    "win_rate_filtered = win_rate[win_rate[\"matches_vs_top10\"] >= 2]\n",
    "\n",
    "# Sort by win rate first, then number of matches\n",
    "best_player = win_rate_filtered.sort_values(\n",
    "    [\"win_percentage_vs_top10\", \"matches_vs_top10\"],\n",
    "    ascending=[False, False]\n",
    ").head(1)\n",
    "\n",
    "players_unique = players_unique[[\"player_id\", \"full_name\", \"current_rank\"]]\n",
    "\n",
    "best_player_named = best_player.reset_index().merge(\n",
    "    players_unique,\n",
    "    on=\"player_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Best player against top 10: {best_player_named['full_name'].values[0]} with ID: {best_player_named['player_id'].values[0]}\")\n",
    "print(\"he have played\", best_player_named['matches_vs_top10'].values[0], \"matches against top 10 players\")\n",
    "print(f\"And he won {best_player_named['wins_vs_top10'].values[0]} of them.\")\n",
    "print(f\"Win rate against top 10: {best_player_named['win_percentage_vs_top10'].values[0]:.2f}%\")\n",
    "print('the number of opponents', len(opponents.drop_duplicates(subset=\"player_id\")))\n",
    "print(\"the number of winning opponents\", len(win_rate.drop_duplicates()))\n",
    "print(f'player info:\\n', players[players[\"player_id\"] == best_player_named['player_id'].values[0]].drop_duplicates(subset=\"player_id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- Best player against top 10: **Rocha, Francisco with ID: 228155**\n",
    "- He have played 2 matches against top 10 players and She won 1 of them.\n",
    "- Win rate against top 10: 50.00% with highest matchs number against top 10\n",
    "\n",
    "#### This statistic does not have a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- There are **40** players as competitors for the top 10 ranked players.\n",
    "- Among the **40** competitors, only **3** were able to win the game against the 10 highest ranked players.\n",
    "- And the best of them is undoubtedly **Rocha, Francisco with ID: 228155**\n",
    "- He is from **Portugal**, **185 cm tall**, weight unknown, current rank **934**.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### Question 17 ‚Äî What is the average number of breaks of serve per match?\n",
    "\n",
    "To calculate the average break of serve per match, we only need the data set `power_final.csv`.\n",
    "\n",
    "In this data, each row is for a game, there is a column called `break_occurred` that indicates whether a break occurred in that game.\n",
    "\n",
    "#### **Steps**\n",
    "1. Reading a dataset and saving it to a dataframe\n",
    "2. Extract `break_occurred`s by grouping based on `match_id`, `set_num`, and `game_num` to get the breaks for each game, and with the help of the `any` function, we can understand the result of having a break or not when dealing with duplicate data.\n",
    "3. Grouping breaks per game based on `match_id` and summing breaks per match\n",
    "4. Averaging games per match\n",
    "5. Filtering outlier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power = pd.read_csv(os.path.join(etl_files_path, \"power_final.csv\"))\n",
    "\n",
    "breaks_per_game = df_power.groupby(['match_id', 'set_num', 'game_num'])['break_occurred'].any()\n",
    "\n",
    "breaks_per_game = breaks_per_game.groupby('match_id').sum()\n",
    "\n",
    "total_avg = breaks_per_game.mean()\n",
    "\n",
    "breaks_per_game = breaks_per_game[(breaks_per_game <= 20) & (breaks_per_game > 0)]\n",
    "\n",
    "\n",
    "print(\"Average breaks per match (excluding outliers):\", total_avg)\n",
    "print(\"Max breaks in a match (for filtered data):\", breaks_per_game.max())\n",
    "print(\"Total matches considered:\", len(breaks_per_game))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### **Final Answer**\n",
    "- The average number of breaks per match is **7.59**\n",
    "\n",
    "#### This dataset does not have any NaN data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "sns.histplot(\n",
    "            x=breaks_per_game,\n",
    "            bins=20,\n",
    "            kde=True,\n",
    "            ax=ax,\n",
    "            color='skyblue',\n",
    "            label='Breaks per Match',\n",
    "            edgecolor='gray',\n",
    "            alpha=0.7)\n",
    "\n",
    "ax.set_title('Distribution of Breaks per Match', loc='left', fontsize=16, color='black', pad=20)\n",
    "ax.set_xlabel('Number of Breaks', loc='left')\n",
    "ax.set_ylabel('Number of Matches', loc='bottom')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.legend(loc='upper left')\n",
    "ax.tick_params(axis='both', colors='gray', labelsize=10)\n",
    "ax.set_xticks(range(0, 21))\n",
    "ax.axvline(total_avg, color='red', linestyle='--', label='Mean Breaks per Match')\n",
    "ax.text(total_avg, ax.get_ylim()[1]*0.9, f'Mean: {total_avg:.2f}',\n",
    "        rotation=90, color='red', verticalalignment='top', horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "The statistics shows:\n",
    "- About 10,968 matches were analyzed in this statistic, with the highest number of breaks per match being 20 breaks per match.\n",
    "- A very small number of matches had 20 breaks per match, which were removed from the statistics as outlier data, and the data was kept up to 20 for the sake of the beauty of the distribution.\n",
    "\n",
    "--------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
