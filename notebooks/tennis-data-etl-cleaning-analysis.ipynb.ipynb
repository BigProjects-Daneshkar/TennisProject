{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Reading Section:\n",
    "##### In this section, we will use the following codes, which will be explained below, to read the data we need from the main reference file and convert it to CSV format so that we can use it in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è IMPORTANT ‚Äî READ BEFORE RUNNING\n",
    "\n",
    "This notebook expects the raw dataset to be available **before execution**.  \n",
    "If the required ZIP file is not placed in the correct path, the ETL pipeline will fail or generate incomplete/duplicated outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Required Action\n",
    "\n",
    "Please make sure the following file exists **before running the notebook**:\n",
    "\n",
    "..\\data\\raw\\tennis_data.zip\n",
    "\n",
    "\n",
    "> üìå The path is already configured inside the notebook‚Äôs Python extraction script ‚Äî do **not** change it unless necessary.\n",
    "\n",
    "If the ZIP file has a different name, please rename it or update the code accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 1: Importing the required libraries, defining the paths, and creating the required directories if they do not exist.\n",
    "In this section, we import the libraries and items we need to use them later, and then we define the main paths, such as the main zip file path, the output file directory, and the temp directory, in a relational manner, to be included in the data folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Define paths\n",
    "main_zip = \"./data/raw/tennis_data.zip\"\n",
    "output_dir = \"./data/processed\"\n",
    "temp_dir = \"./data/raw/temp\"\n",
    "base_path = \"./data/processed\"\n",
    "clean_path = \"./data/processed/clean\"\n",
    "\n",
    "os.makedirs(clean_path, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Part 2: Creating a CSV table generator function from Parquet files\n",
    "In this section, we have created a very useful function that, based on the keyword of the parquet category name that we give it, goes to the defined path of the main zip file and reads the parquets belonging to the specified tables and the data related to the specified columns from the zip files for each day. In addition to all this, we specify that the records of this table should be unique based on the unique data identifier or that this table can have multiple rows for each unique identifier. Our unique identifier is match_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(table_keyword, needed_cols, output_name, dedup_on=\"match_id\"):\n",
    "    \"\"\"\n",
    "    table_keyword: like 'event_' or 'home_team_'\n",
    "    needed_cols: list of needed columns\n",
    "    output_name: name of output CSV file\n",
    "    dedup_on: unique column for deduplication (default is 'match_id')\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(output_dir, output_name)\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    all_dfs = []\n",
    "    row_counter = 0\n",
    "\n",
    "    with zipfile.ZipFile(main_zip, \"r\") as main_zip_ref:\n",
    "        daily_zips = main_zip_ref.namelist()\n",
    "        print(f\"üì¶ Count of daily zips: {len(daily_zips)}\")\n",
    "\n",
    "        for i, daily_zip_name in enumerate(daily_zips, start=1):\n",
    "            print(f\"üîπ ({i}/{len(daily_zips)}) processing {daily_zip_name} ...\")\n",
    "            main_zip_ref.extract(daily_zip_name, temp_dir)\n",
    "            daily_zip_path = os.path.join(temp_dir, daily_zip_name)\n",
    "\n",
    "            with zipfile.ZipFile(daily_zip_path, \"r\") as daily_zip_ref:\n",
    "                parquet_files = [f for f in daily_zip_ref.namelist() if f.endswith(\".parquet\") and table_keyword in f]\n",
    "                for f in parquet_files:\n",
    "                    with daily_zip_ref.open(f) as pf:\n",
    "                        table = pq.read_table(BytesIO(pf.read()))\n",
    "                        df = table.to_pandas()\n",
    "                        df = df[[c for c in needed_cols if c in df.columns]]\n",
    "                        df[\"date_source\"] = daily_zip_name.replace(\".zip\", \"\")\n",
    "                        all_dfs.append(df)\n",
    "                        row_counter += len(df)\n",
    "\n",
    "            os.remove(daily_zip_path)\n",
    "\n",
    "    if all_dfs:\n",
    "        df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Shape: {df_all.shape}\")\n",
    "        if dedup_on and dedup_on in df_all.columns:\n",
    "            df_all = df_all.drop_duplicates(subset=dedup_on)\n",
    "        else:\n",
    "            df_all = df_all.drop_duplicates()\n",
    "        print(f\"üßπ after cleaning duplicated rows: {df_all.shape}\")\n",
    "\n",
    "        df_all.to_csv(csv_path, index=False)\n",
    "        print(f\"üíæ Saved: {csv_path}\")\n",
    "        print(f\"üìä Count of all rows: {len(df_all)}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è There is no file for {table_keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Part 3: Using the above cell function and creating CSVs of the tables required for analysis according to the columns required from them\n",
    "In this part, based on the initial analysis we had of the 17 questions in question and the data they required, we extracted a series of tables from a total of 15 tables and a series of their columns that were needed to analyze and answer the 17 questions we needed. Here, we want to extract them from the original raw zip file and convert them to CSV files so that we can use these files later in analyzing and answering the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_table(\n",
    "    table_keyword=\"event_\",\n",
    "    needed_cols=[\"match_id\", \"first_to_serve\", \"winner_code\", \"default_period_count\", \"start_datetime\", \"match_slug\"],\n",
    "    output_name=\"event.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"home_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"home_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"away_team_\",\n",
    "    needed_cols=[\"match_id\", \"player_id\", \"full_name\", \"gender\", \"height\", \"weight\", \"plays\", \"current_rank\", \"country\"],\n",
    "    output_name=\"away_team.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"tournament_\",\n",
    "    needed_cols=[\"match_id\", \"tournament_id\", \"tournament_name\", \"ground_type\", \"tennis_points\", \"start_datetime\"],\n",
    "    output_name=\"tournament.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"time_\",\n",
    "    needed_cols=[\"match_id\", \"period_1\", \"period_2\", \"period_3\", \"period_4\", \"period_5\", \"current_period_start_timestamp\"],\n",
    "    output_name=\"time.csv\",\n",
    "    dedup_on=\"match_id\"\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"statistics_\",\n",
    "    needed_cols=[\"match_id\", \"statistic_name\", \"home_value\", \"away_value\"],\n",
    "    output_name=\"statistics.csv\",\n",
    "    dedup_on=None  # No deduplication because we have multiple rows per match_id in statistics\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"power_\",\n",
    "    needed_cols=[\"match_id\", \"set_num\", \"game_num\", \"value\", \"break_occurred\"],\n",
    "    output_name=\"power.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in power\n",
    ")\n",
    "\n",
    "build_table(\n",
    "    table_keyword=\"pbp_\",\n",
    "    needed_cols=[\"match_id\", \"set_id\", \"game_id\", \"point_id\", \"home_point\", \"away_point\", \"home_score\"],\n",
    "    output_name=\"pbp.csv\",\n",
    "    dedup_on=None # No deduplication because we have multiple rows per match_id in pbp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "##  Part 4: Data Cleaning Stage\n",
    "#### In this section, we will clean the extracted CSV files created in the previous section.\n",
    "# \n",
    "### **Goal:**  \n",
    "- Remove duplicate rows  \n",
    " - Handle missing values (`NaN`)  \n",
    " - Standardize data types  \n",
    " The cleaned outputs will be stored in `../data/clean` for the next normalization phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "###  Cleaning: Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv(os.path.join(base_path, \"event.csv\"))\n",
    "df_event.drop_duplicates(inplace=True)\n",
    "\n",
    "for col in df_event.columns:\n",
    "    if df_event[col].dtype == 'object':\n",
    "        df_event[col] = df_event[col].fillna(\"Unknown\")\n",
    "    else:\n",
    "        df_event[col] = df_event[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_event.columns:\n",
    "    df_event[\"match_id\"] = df_event[\"match_id\"].astype(str)\n",
    "\n",
    "df_event.to_csv(os.path.join(clean_path, \"event_clean.csv\"), index=False)\n",
    "print(\"‚úÖ event_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "###  Cleaning: Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home = pd.read_csv(os.path.join(base_path, \"home_team.csv\"))\n",
    "df_home = df_home.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(str)\n",
    "\n",
    "df_home.to_csv(os.path.join(clean_path, \"home_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ home_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "###  Cleaning: Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_away = pd.read_csv(os.path.join(base_path, \"away_team.csv\"))\n",
    "df_away = df_away.drop_duplicates()\n",
    "\n",
    "string_cols = [\"full_name\", \"gender\", \"plays\", \"country\"]\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(0)\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(str)\n",
    "\n",
    "df_away.to_csv(os.path.join(clean_path, \"away_team_clean.csv\"), index=False)\n",
    "print(\"‚úÖ away_team_clean.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Part 5: Normalization Stage\n",
    "#### Now that we have clean CSVs, in this part we will:\n",
    "#\n",
    " - Convert data types (e.g., timestamps to datetime)  \n",
    " - Standardize text (e.g., capitalization, spacing)  \n",
    " - Fill remaining missing values intelligently (using mean, median, or mode)  \n",
    " The normalized final datasets will be saved in `../data/clean` as `_final.csv` files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./data/processed\"\n",
    "input_path = os.path.join(base_path, \"event_clean.csv\")\n",
    "output_path = os.path.join(base_path, \"event_final.csv\")\n",
    "\n",
    "df_event = pd.read_csv(input_path)\n",
    "\n",
    "df_event[\"match_id\"] = df_event[\"match_id\"].astype(int)\n",
    "df_event[\"default_period_count\"] = df_event[\"default_period_count\"].astype(int)\n",
    "df_event[\"date_source\"] = df_event[\"date_source\"].astype(int)\n",
    "\n",
    "if np.issubdtype(df_event[\"start_datetime\"].dtype, np.number):\n",
    "    df_event[\"start_datetime\"] = pd.to_datetime(df_event[\"start_datetime\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "df_event[\"winner_code\"] = df_event[\"winner_code\"].fillna(df_event[\"winner_code\"].mode()[0])\n",
    "df_event[\"first_to_serve\"] = df_event[\"first_to_serve\"].fillna(df_event[\"first_to_serve\"].mode()[0])\n",
    "\n",
    "df_event.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ event_final.csv created successfully!\")\n",
    "print(df_event.info())\n",
    "print(df_event.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Home Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(base_path, \"home_team_clean.csv\")\n",
    "output_path = os.path.join(base_path, \"home_team_final.csv\")\n",
    "\n",
    "df_home = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = pd.to_numeric(df_home[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_home.columns:\n",
    "    df_home[\"gender\"] = df_home[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_home.columns:\n",
    "    df_home[\"plays\"] = df_home[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_home.columns:\n",
    "    df_home[\"height\"] = df_home[\"height\"].fillna(df_home[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_home.columns:\n",
    "    df_home[\"weight\"] = df_home[\"weight\"].fillna(df_home[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_home.columns:\n",
    "    df_home[\"current_rank\"] = df_home[\"current_rank\"].fillna(df_home[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_home.columns:\n",
    "        mode_val = df_home[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_home[col] = df_home[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_home.columns:\n",
    "        df_home[col] = df_home[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_home.columns:\n",
    "    df_home[\"match_id\"] = df_home[\"match_id\"].astype(str)\n",
    "\n",
    "df_home.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ home_team_final.csv created successfully!\")\n",
    "print(df_home.info())\n",
    "print(df_home.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "###  Normalization ‚Äî Away Team Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(base_path, \"away_team_clean.csv\")\n",
    "output_path = os.path.join(base_path, \"away_team_final.csv\")\n",
    "\n",
    "df_away = pd.read_csv(input_path)\n",
    "\n",
    "numeric_cols = [\"height\", \"weight\", \"current_rank\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = pd.to_numeric(df_away[col], errors=\"coerce\")\n",
    "\n",
    "if \"gender\" in df_away.columns:\n",
    "    df_away[\"gender\"] = df_away[\"gender\"].astype(str).str.strip().str.title().replace({\"Nan\":\"Unknown\"})\n",
    "if \"plays\" in df_away.columns:\n",
    "    df_away[\"plays\"] = df_away[\"plays\"].astype(str).str.strip().str.lower().replace({\"nan\":\"unknown\"})\n",
    "for col in [\"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].astype(str).str.strip()\n",
    "\n",
    "if \"height\" in df_away.columns:\n",
    "    df_away[\"height\"] = df_away[\"height\"].fillna(df_away[\"height\"].mean(skipna=True))\n",
    "if \"weight\" in df_away.columns:\n",
    "    df_away[\"weight\"] = df_away[\"weight\"].fillna(df_away[\"weight\"].mean(skipna=True))\n",
    "if \"current_rank\" in df_away.columns:\n",
    "    df_away[\"current_rank\"] = df_away[\"current_rank\"].fillna(df_away[\"current_rank\"].median(skipna=True))\n",
    "\n",
    "for col in [\"gender\", \"plays\"]:\n",
    "    if col in df_away.columns:\n",
    "        mode_val = df_away[col].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df_away[col] = df_away[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "for col in [\"player_id\", \"full_name\", \"country\"]:\n",
    "    if col in df_away.columns:\n",
    "        df_away[col] = df_away[col].fillna(\"Unknown\")\n",
    "\n",
    "if \"match_id\" in df_away.columns:\n",
    "    df_away[\"match_id\"] = df_away[\"match_id\"].astype(str)\n",
    "\n",
    "df_away.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ away_team_final.csv created successfully!\")\n",
    "print(df_away.info())\n",
    "print(df_away.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "##  Part 6: Review and Summary\n",
    "#### In this final part, we review all steps in the data preparation phase:\n",
    "# \n",
    "| Step | Description | Output Folder | Key Action |\n",
    "|------|--------------|----------------|-------------|\n",
    "| 1 | Extraction from Parquet (Raw) | `../data/raw` | `build_table()` function |\n",
    "| 2 | Cleaning | `../data/processed` | Remove duplicates, fill NaN with neutral values |\n",
    "| 3 | Normalization | `../data/processed/clean` | Type casting, smart imputation |\n",
    "# \n",
    "####  All datasets are now ready for **Phase 2 (Data Integration)**, where we will merge and build the central match table for analysis.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
